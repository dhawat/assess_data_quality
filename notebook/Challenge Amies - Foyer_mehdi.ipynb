{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edcd758",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "from pyod.models.copod import COPOD \n",
    "from pyod.models.iforest import IForest\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba41e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import prince\n",
    "import ipdb\n",
    "\n",
    "\n",
    "class Data:\n",
    "    \"\"\"Data class holding a column by column profile and index flagged as low quality data\"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (CSV, JSON, SQL): data set.\n",
    "        \"\"\"\n",
    "        if check_extension(path) == \"none\":\n",
    "            raise TypeError(\"data should be of provided as .csv or .json or .sql file\")\n",
    "\n",
    "        self.data = _to_DataFrame(path)\n",
    "        self._profile = None\n",
    "        self._good_index = list(range(self.data.shape[0]))\n",
    "        self._bad_index = pd.DataFrame(columns=['idx', 'column', 'errtype'])\n",
    "\n",
    "    @property\n",
    "    def profile(self):\n",
    "        \"\"\"getter for private attribute _profile\n",
    "\n",
    "        Raises:\n",
    "            Exception: If profile is not yet initialize it raise an error.\n",
    "\n",
    "        Returns:\n",
    "            Object: Profile object\n",
    "        \"\"\"\n",
    "        if self._profile is None:\n",
    "            raise Exception('profile is None')\n",
    "        return self._profile\n",
    "    \n",
    "    def set_profile(self):\n",
    "        \"\"\"profile setter, to use after initializing the instance.\n",
    "        \"\"\"\n",
    "        profile = {column: Profile(self, column) for column in self.data.columns}\n",
    "        self._profile = profile\n",
    "\n",
    "    @property\n",
    "    def good_index(self):\n",
    "        \"\"\"getter for private attribute _good_index\n",
    "\n",
    "        Returns:\n",
    "            list: list of good indexes to use for ML training purposes\n",
    "        \"\"\"\n",
    "        return self._good_index\n",
    "\n",
    "    @property\n",
    "    def bad_index(self):\n",
    "        \"\"\"getter for private attribute for _bad_index\n",
    "\n",
    "        Returns:\n",
    "            dataFrame: dataFrame containing error indexes and if applicable column and an explaination of the error\n",
    "        \"\"\"\n",
    "        return self._bad_index\n",
    "    \n",
    "    @bad_index.setter\n",
    "    def bad_index(self, list_idx):\n",
    "        \"\"\"setter if private attribute bad_index\n",
    "\n",
    "        Args:\n",
    "            list_idx (dataFrame): used as bad_index = bad_index.append(df)\n",
    "        \"\"\"\n",
    "        self._bad_index = list_idx\n",
    "\n",
    "    @good_index.setter\n",
    "    def good_index(self, list_idx):\n",
    "        \"\"\"setter for private attribute _good_index\n",
    "\n",
    "        Args:\n",
    "            list_idx (list): list of good index to replace the previous one\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if length is greater than the initial dataFrame raises Valueerror\n",
    "        \"\"\"\n",
    "        if len(list_idx) > self.data.shape[0]:\n",
    "            raise ValueError('Index length must be smaller than the length of the dataframe')\n",
    "        self._good_index = list_idx\n",
    "\n",
    "    def get_str_col(self):\n",
    "        \"\"\"return names of string columns of the dataFrame, raises an exception if profile is not set.\n",
    "\n",
    "        Returns:\n",
    "            [list]: list of string columns\n",
    "        \"\"\"\n",
    "        col_list = []\n",
    "        for column in self.data.columns:\n",
    "            if self.profile[column]._col_type == type(str()):\n",
    "                col_list.append(column)\n",
    "        return col_list\n",
    "\n",
    "    def get_nbr_col(self):\n",
    "        \"\"\"return names of number columns of the dataFrame, raises an exception if profile is not set.\n",
    "\n",
    "        Returns:\n",
    "            [list]: list of number columns\n",
    "        \"\"\"\n",
    "        col_list = []\n",
    "        for column in self.data.columns:\n",
    "            if self.profile[column]._col_type in [type(int()), type(float())]:\n",
    "                col_list.append(column)\n",
    "        return col_list\n",
    "        \n",
    "    def push_bad_index(self, list_idx): #Find a better method name\n",
    "        \"\"\"decrepated, not sure if will be used or not.\n",
    "\n",
    "        Args:\n",
    "            list_idx ([type]): [description]\n",
    "        \"\"\"\n",
    "        for elem in list_idx:\n",
    "            try:\n",
    "                self.bad_index.append(elem)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def firstpass(self):\n",
    "        \"\"\"Push into self.bad_index the indexes and error types of data.\n",
    "        This first pass detects duplicated data, typo and extreme values\n",
    "        \"\"\"\n",
    "        # Deterministic pass\n",
    "        n_duped_idx = ~_duplicated_idx(self.data)\n",
    "\n",
    "        for index in n_duped_idx[~n_duped_idx].index.values.tolist():\n",
    "            self.bad_index = self.bad_index.append({'idx': index, 'column': 'All', 'errtype': 'duplication'}, ignore_index=True)\n",
    "        \n",
    "        # Probabilistic pass\n",
    "        for column in self.get_str_col(): # Columns of strings only\n",
    "            if data.profile[column]._uniqueness <= 0.005: # Filter column with too many different words\n",
    "                clean_df = self.data[n_duped_idx]\n",
    "                clean_df = clean_df[column][clean_df[column].notna().values]\n",
    "                idx = index_uncorrect_grammar(clean_df) #get the non duped indexes and not na from a column\n",
    "                idx = clean_df.iloc[idx].index\n",
    "                \n",
    "                for index in idx:\n",
    "                    row = {'idx': index, 'column': column, 'errtype': 'typo'}\n",
    "                    self.bad_index = self.bad_index.append(row, ignore_index=True)\n",
    "\n",
    "        for column in self.get_nbr_col(): # Columns of numbers only\n",
    "            clean_df = self.data[n_duped_idx]\n",
    "            clean_df = clean_df[column][clean_df[column].notna().values]\n",
    "            idx = proba_model(clean_df, self.profile[column]._mean, self.profile[column]._std)\n",
    "            idx = clean_df[idx].index\n",
    "\n",
    "            for index in idx:\n",
    "                row = {'idx': index, 'column': column, 'errtype': 'extreme value'}\n",
    "                self.bad_index = self.bad_index.append(row, ignore_index=True)\n",
    "\n",
    "class Profile:\n",
    "    \"\"\"A profile for a dataframe column.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Data, column):\n",
    "        self._emptiness = _is_none(Data.data, column)\n",
    "        self._size = Data.data[column].shape[0]\n",
    "        self._uniqueness = _is_unique(Data.data, column)\n",
    "        self._col_type = check_data_type(Data.data[column])\n",
    "        if self._col_type == type(str()):\n",
    "            pass\n",
    "        if self._col_type in [type(int()), type(float())]:\n",
    "            self._min = Data.data[column].min()\n",
    "            self._max = Data.data[column].max()\n",
    "            self._mean = Data.data[column].mean()\n",
    "            self._std = Data.data[column].std()\n",
    "\n",
    "    @property\n",
    "    def emptiness(self):\n",
    "        \"\"\"getter of private attribute _emptiness\n",
    "\n",
    "        Returns:\n",
    "            [float]: ratio of na inside column\n",
    "        \"\"\"\n",
    "        return self._emptiness\n",
    "\n",
    "    @emptiness.setter\n",
    "    def emptiness(self, value):\n",
    "        \"\"\"setter of private attribute _emptiness\n",
    "\n",
    "        Args:\n",
    "            value (float): ratio of na inside column\n",
    "        \"\"\"\n",
    "        self._emptiness = value\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        \"\"\"getter of private attribute _size\n",
    "\n",
    "        Returns:\n",
    "            [int]: size of the column\n",
    "        \"\"\"\n",
    "        return self._size\n",
    "\n",
    "    @size.setter\n",
    "    def size(self, value):\n",
    "        \"\"\"setter of private attribute _size\n",
    "\n",
    "        Args:\n",
    "            value ([int]): size of the column \n",
    "        \"\"\" # To note : make truly private\n",
    "        self._size = value\n",
    "\n",
    "    @property\n",
    "    def uniqueness(self):\n",
    "        \"\"\"getter of private attribute _uniqueness\n",
    "\n",
    "        Returns:\n",
    "            [float]: ratio of unique element inside column\n",
    "        \"\"\"\n",
    "        return self._uniqueness\n",
    "\n",
    "    @uniqueness.setter\n",
    "    def uniqueness(self, value):\n",
    "        self._uniqueness = value\n",
    "\n",
    "    @property\n",
    "    def col_type(self):\n",
    "        \"\"\"getter of private attribute _col_type\n",
    "\n",
    "        Returns:\n",
    "            [type]: returns type python object of the type of the column \n",
    "        \"\"\"\n",
    "        return self._col_type\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"df = pd.read_csv(\"logs.csv\")  # read data\n",
    "\n",
    "df = df.set_index(\"d\")  # to re-index with a column 'd'\n",
    "df = df.sort_index()  # to sort with respect to the index \"\"\"\n",
    "\n",
    "\n",
    "def check_extension(data):\n",
    "    \"\"\"check if the extension of data is within CSV, JSON or SQL\n",
    "\n",
    "    Args:\n",
    "        data (): data set\n",
    "\n",
    "    Returns:\n",
    "        type of allowed extension or none.\n",
    "    \"\"\"\n",
    "    if re.search(\"\\.csv$\", data, flags=re.IGNORECASE):\n",
    "        return \"csv\"\n",
    "    if re.search(\"\\.json$\", data, flags=re.IGNORECASE):\n",
    "        return \"json\"\n",
    "    if re.search(\"\\.sql$\", data, flags=re.IGNORECASE):\n",
    "        return \"sql\"\n",
    "    if re.search(\"\\.xlsx$\", data, flag=re.IGNORECASE):\n",
    "        return \"xlsx\"\n",
    "    return \"none\"\n",
    "\n",
    "\n",
    "def _to_DataFrame(data):\n",
    "    \"\"\"read data and transform it to DataFrame\n",
    "\n",
    "    Args:\n",
    "        data (csv, json, sql, xlsx): data\n",
    "\n",
    "    Returns:\n",
    "        Dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    ext = check_extension(data)\n",
    "    assert ext != \"none\"\n",
    "    f_dict = {\n",
    "        \"csv\": pd.read_csv,\n",
    "        \"json\": pd.read_json,\n",
    "        \"sql\": pd.read_sql,\n",
    "        \"xlsx\": pd.read_excel,\n",
    "    }\n",
    "    df = f_dict[ext](data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_metadata(df):\n",
    "    \"\"\"read a dataframe and generate relevant metadata such as columns types etc\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): data\n",
    "\n",
    "    Returns:\n",
    "        dict: {name_of_column: metadata_associated}\n",
    "    \"\"\"\n",
    "    metadata = []\n",
    "    for column in df:\n",
    "        metadata.append(check_data_type(df[column]))\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def check_data_type(column):\n",
    "    \"\"\"check type in a column which type is it using a voting method from all the non na data\n",
    "\n",
    "    Args:\n",
    "        column (pandas.core.series.Series): column from a dataframe\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    types_dict = {}\n",
    "    for e in column[column.notna()]:\n",
    "        if type(e) not in types_dict:\n",
    "            types_dict[type(e)] = 1\n",
    "        else:\n",
    "            types_dict[type(e)] += 1\n",
    "    if len(types_dict) != 0:\n",
    "        return max(types_dict, key=types_dict.get)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "\n",
    "def _is_date(string, fuzzy=False):\n",
    "    \"\"\"check if a given string is a date and return the date if true and raise a ValueError if false\n",
    "\n",
    "    Args:\n",
    "        string (string): string to check\n",
    "        fuzzy (bool, optional): Enable a more lenient search in the string. Defaults to False.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: raised when string is not likely to be a date\n",
    "\n",
    "    Returns:\n",
    "        string: datetime as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pd.to_datetime(string)\n",
    "        return pd.to_datetime(string)\n",
    "\n",
    "    except ValueError:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "def _is_duplicated(df):\n",
    "    \"\"\"Find duplicated row and return dataframe without the duplication\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): data frame\n",
    "\n",
    "    Returns:\n",
    "        duplicated_row: the duplicated rows\n",
    "        df_clean: the DataFrame without the duplicated rows\n",
    "    \"\"\"\n",
    "    df_new = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "    duplicated_row = df[df_new.duplicated()]  # duplicated row\n",
    "    df_clean = df[~df_new.duplicated()]  # without duplication row\n",
    "    return df_clean, duplicated_row\n",
    "\n",
    "\n",
    "def _is_unique(df, col_name=\"\"):\n",
    "    \"\"\"verify uniqueness over a specified column, and find the uniqueness coefficient\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Data Frame.\n",
    "        col_name (str, optional): Column name. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        ratio : 1 - (number of repeated data in a column)/card(the column)\n",
    "                if 1 means all values are unique\n",
    "    \"\"\"\n",
    "    df_clean, _ = _is_duplicated(df)\n",
    "    return df_clean[col_name].nunique() / df_clean[col_name].shape[0]\n",
    "\n",
    "\n",
    "def _is_none(df, col_name=\"\"):\n",
    "    \"\"\"find none ratio in a specific columns\n",
    "\n",
    "    Args:\n",
    "        df ([type]): [description]\n",
    "        col_name (str, optional): [description]. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        ratio: none ration in the columns\n",
    "                1 means all the columns is none\n",
    "                0 means non none\n",
    "    \"\"\"\n",
    "    df_clean, _ = _is_duplicated(df)\n",
    "    none_element = df_clean[col_name][df_clean[col_name].isnull()]\n",
    "    ratio = len(none_element) / df_clean.shape[0]\n",
    "    return ratio\n",
    "\n",
    "\n",
    "def proba_model(col, mean, std, tresh=6):\n",
    "    \"\"\"cutting distribution between mean-3*std and mean+3*std\n",
    "\n",
    "    Args:\n",
    "        df ([type]): [description]\n",
    "        col_name ([type]): [description]\n",
    "        mean ([type]): [description]\n",
    "        std ([type]): [description]\n",
    "        tresh (int, optional): [description]. Defaults to 6.\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    upper_bound = mean + tresh * std\n",
    "    lower_bound = mean - tresh * std\n",
    "    idx = col[\n",
    "        ~((col > lower_bound) * (col < upper_bound))\n",
    "    ].index  # trancate values from the column\n",
    "      # clean dataframe\n",
    "    return idx\n",
    "\n",
    "# Possibilité d'améliorer\n",
    "# Threshold for anomalie is fixed at Q_1 = round(np.percentile(unique_counts, 5)), could be improved. \n",
    "# DBSCAN for example on the number of occurences on words.\n",
    "\n",
    "def uncorrect_grammar(df_names, cluster, min_occurence):\n",
    "    \"\"\"index of element\n",
    "\n",
    "    Args:\n",
    "        df_names ([type]): [description]\n",
    "        cluster ([type]): [description]\n",
    "        min_occurence (int): [min # of répétition of a label to be considered an error]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    words = np.asarray(df_names)\n",
    "    unique_words, unique_counts = np.unique(df_names, return_counts=True)\n",
    "    index_In_words = []\n",
    "    for w in cluster:\n",
    "        count = unique_counts[np.where(unique_words == w)[0]][0]\n",
    "        if count <= min_occurence:\n",
    "            index_In_words = index_In_words + np.ndarray.tolist(np.where(words == w)[0])\n",
    "    return index_In_words\n",
    "\n",
    "\n",
    "def index_uncorrect_grammar(df_State):\n",
    "    df_State_unique = np.unique(df_State)\n",
    "    words = np.asarray(df_State_unique) #So that indexing with a list will work\n",
    "    lev_similarity = np.array([[SequenceMatcher(None, w1, w2).ratio() for w1 in words] for w2 in words])\n",
    "    affprop = AffinityPropagation(affinity = \"precomputed\", damping=0.5)\n",
    "    affprop.fit(lev_similarity)\n",
    "    list_uncorrect = []\n",
    "    if len(np.unique(affprop.labels_)) == 1:\n",
    "        return list_uncorrect\n",
    "    else:\n",
    "        for cluster_id in np.unique(affprop.labels_):\n",
    "            cluster = np.unique(words[np.nonzero(affprop.labels_ == cluster_id)])\n",
    "            if len(cluster) > 1:\n",
    "                list_uncorrect = list_uncorrect + uncorrect_grammar(df_State, cluster, 10)\n",
    "    return list_uncorrect\n",
    "\n",
    "def _duplicated_idx(df):\n",
    "    df_new = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "    return df_new.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e241600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = Data('improved_data_quality\\data_avec_erreurs_wasserstein.csv')\n",
    "data.set_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93283d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.firstpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82da97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mca = prince.MCA(n_components=2, n_iter=3, copy=True, check_input=True, engine='auto', random_state=42)\n",
    "modified_df = data.data.drop('ProductGroup', axis=1).fillna(method='backfill', axis=1, limit=10).iloc[:2000:20]\n",
    "mca = mca.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    " ax = mca.plot_coordinates(\n",
    "...     X=data.data.drop('ProductGroup', axis=1).fillna(method='backfill', axis=1, limit=10).iloc[:2000:20],\n",
    "...     ax=None,\n",
    "...     figsize=(12, 12),\n",
    "...     show_row_points=True,\n",
    "...     row_points_size=10,\n",
    "...     show_row_labels=False,\n",
    "...     show_column_points=True,\n",
    "...     column_points_size=30,\n",
    "...     show_column_labels=False,\n",
    "...     legend_n_cols=2\n",
    "... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = COPOD()\n",
    "clf.fit(data.data[['Unnamed: 0', 'YearMade']])\n",
    "results = clf.decision_scores_\n",
    "arg = results.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e9fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = IForest()\n",
    "clf.fit(data.data[['Unnamed: 0', 'YearMade']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = clf.decision_scores_.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5088d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data['YearMade'][arg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bde022",
   "metadata": {},
   "outputs": [],
   "source": [
    "for number_table in data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d15c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = proba_model(data.data['SalePrice'][data.data['SalePrice'].notna()], data.profile[2]._mean, data.profile[2]._std, tresh=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(data.data['SalePrice'][res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75441461",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data['fiBaseModel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ac717",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_duped_idx = ~_duplicated_idx(data.data)\n",
    "clean_df = data.data[n_duped_idx]\n",
    "clean_data_na = clean_df['fiModelSeries'][clean_df['fiModelSeries'].notna().values]\n",
    "\n",
    "\n",
    "idx = index_uncorrect_grammar(clean_data_na)\n",
    "\n",
    "#idx = proba_model(clean_data_na, \\\n",
    "#                data.profile['YearMade']._mean, data.profile['YearMade']._std)\n",
    "idx = clean_data_na.iloc[idx].index\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386238a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data['fiModelSeries'].iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de73aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data['fiModelSeries'][data.data['fiModelSeries'].notna().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7bb18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.data['fiSecondaryDesc'].iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb887c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data['fiSecondaryDesc'][data.data['fiSecondaryDesc'].notna().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data['YearMade'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722f06d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = index_uncorrect_grammar(column_test.iloc[column_test.notna().values]) #get the non duped indexes and not na from a column\n",
    "idx = column_test.iloc[~column_test.isna().values].iloc[idx].index\n",
    "idx\n",
    "column_test[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96b6ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_data_type(data.data['YearMade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b5c1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in data.bad_index['idx']:\n",
    "    print(idx)\n",
    "    try:\n",
    "        data.good_index.remove(idx)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.good_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f5a0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_entier_nombre = data.data.iloc[data.good_index].drop('ProductGroup', axis=1).drop('saledate', axis=1)\n",
    "for name_col in data.data.columns:\n",
    "    if data.profile[name_col]._col_type in [type(int()), type(float())]:\n",
    "        df_entier_nombre.drop(name_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3039420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "le.fit(df_entier_nombre.stack(dropna=False).reset_index(drop=True))\n",
    "for col in df_entier_nombre.columns:\n",
    "    df_entier_nombre[col] = le.transform(df_entier_nombre[col])\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(df_entier_nombre)\n",
    "\n",
    "cov = np.abs(pca.get_covariance())\n",
    "variance = np.diag(cov)\n",
    "corr = (cov / np.dot(variance[:, np.newaxis], variance[np.newaxis, :]))\n",
    "\n",
    "\n",
    "df_cov = pd.DataFrame({col: cov[:, idx] for idx, col in enumerate(df_entier_nombre.columns)}, index=[col for col in df_entier_nombre.columns])\n",
    "df_corr = pd.DataFrame({col: corr[:, idx] for idx, col in enumerate(df_entier_nombre.columns)}, index=[col for col in df_entier_nombre.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec76a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (40, 40)\n",
    "plt.pcolor(df_cov, cmap='gray')\n",
    "plt.yticks(np.arange(0.5, len(df_cov.index), 1), df_cov.index)\n",
    "plt.xticks(np.arange(0.5, len(df_cov.columns), 1), df_cov.columns)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20131826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import linalg, eye, csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "csr_matrix(1 - pairwise_distances(df_entier_nombre['state'], metric='hamming'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _col_str_to_nb(col):\n",
    "    matrix = np.array([[SequenceMatcher(None, w1, w2).ratio() for w1 in col] for w2 in col]).sum(axis=0)\n",
    "    dict_pairwise = {key: value for key, value in zip(col, matrix)}\n",
    "    return dict_pairwise\n",
    "_col_str_to_nb(df_entier_nombre[cols[0]].stack(dropna=True).reset_index(drop=True).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2489671",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entier_nombre[cols[2]].stack(dropna=True).reset_index(drop=True).unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c9a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = []\n",
    "for i, col in enumerate(df_cov.columns):\n",
    "    print(col)\n",
    "    cols.append(df_cov.columns[np.abs(cov[:, i]) > np.abs(cov[:, i]).mean() + 0.5*np.abs(cov[:, i]).std()].values.tolist())\n",
    "    if col not in cols[i]:\n",
    "        cols[i].append(col)\n",
    "    print(cols[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = IForest(contamination = 0.01, n_jobs = -1)\n",
    "\n",
    "clf.fit(df_entier_nombre[cols[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a41e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = COPOD()\n",
    "clf.fit(df_entier_nombre[cols[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = clf.predict_proba(df_entier_nombre[cols[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_idx2 = np.where(proba[:, 1] >= 0.95)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ced2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_idx = clf.predict(df_entier_nombre[cols[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d6424c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.data.iloc[data.good_index].iloc[bd_idx2][cols[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d690bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.data.iloc[data.good_index][bd_idx == 0][cols[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df1b3a",
   "metadata": {},
   "source": [
    "# Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c54d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_document = data.data.iloc[data.good_index].drop('ProductGroup', axis=1).drop('saledate', axis=1)\n",
    "for name_col in data.data.columns:\n",
    "    if data.profile[name_col]._col_type in [type(int()), type(float())]:\n",
    "        df_document.drop(name_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c90370",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = []\n",
    "for name_col in df_document.columns:\n",
    "    document.append(df_document[name_col].fillna('Nan').tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd7f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(document)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1732cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.LsiModel(corpus, id2word=dictionary, num_topics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc8bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lsi = model[corpus]\n",
    "\n",
    "for doc, as_text in zip(corpus_lsi, documents):\n",
    "    print(doc, as_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff3edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "df_document = data.data.iloc[data.good_index].drop('ProductGroup', axis=1).drop('saledate', axis=1)\n",
    "for name_col in data.data.columns:\n",
    "    if data.profile[name_col]._col_type in [type(int()), type(float())]:\n",
    "        df_document.drop(name_col, axis=1, inplace=True)\n",
    "document = []\n",
    "for name_col in df_document.columns:\n",
    "    document.append(df_document[name_col].fillna('Nan').tolist())\n",
    "tokenized_sentences = document\n",
    "model = model = Word2Vec(tokenized_sentences, vector_size=100, window=2, min_count=2, workers=6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a7e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _string_to_nbr(df):\n",
    "    \"\"\"Convert a DataFrame (which may have multiple columns) of strings into a return df\n",
    "    with vectors inside.\n",
    "    Args:\n",
    "        df ([DataFrame]): [DataFrame containing strings]\n",
    "    Returns:\n",
    "        [DataFrame]: [DataFrame converted into vectors]\n",
    "    \"\"\"\n",
    "    document = []\n",
    "    for col in df.columns:\n",
    "        document.append(df[col].dropna().tolist())\n",
    "    tokenized_sentences = document\n",
    "    model = Word2Vec(tokenized_sentences, vector_size=100, window=2, min_count=0, workers=6)\n",
    "    df = df.applymap(lambda x: map_model(model, x))\n",
    "    import ipdb; ipdb.set_trace()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74bb0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_model(model, x):\n",
    "    try:\n",
    "        return model.wv[x]\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b21e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv['Medium'])\n",
    "print(model.wv.most_similar('Medium', topn=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_document.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_document\n",
    "df_nb = _string_to_nbr(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32aaacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e8c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glove_vectors.most_similar('excavator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = [[col] for col in data.data.columns.values]\n",
    "tokenized_sentences_col = cols\n",
    "lsi = Word2Vec(tokenized_sentences_col, vector_size=20, window=2, min_count=0, workers=6)\n",
    "\n",
    "print(lsi.wv.most_similar('saledate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad87e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(lsi.wv.most_similar('YearMade'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cadf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tendancy_detection(df, thresh):\n",
    "    \"\"\"\n",
    "    df = dataframe\n",
    "    thresh = threshold for the tendancy detection \n",
    "    return:\n",
    "            dictionnaire with the pair of columns and the anomaly index detected.\n",
    "    \"\"\"\n",
    "    dictionnaire_anomalie_tendance = {}\n",
    "    \n",
    "    for w1 in df:\n",
    "        for w2 in df:\n",
    "            range_anomalie = 0\n",
    "            proportion = np.shape(np.where(df[w1] <  df[w2])[0])[0]/len(df)\n",
    "            if  proportion > thresh:\n",
    "                range_anomalie = np.shape(np.where(df[w1] > df[w2])[0])[0]\n",
    "                if range_anomalie > 0:\n",
    "                    dictionnaire_anomalie_tendance[(w1, w2)] = np.ndarray.tolist(np.where(df[w1] > df[w2])[0])\n",
    "    return dictionnaire_anomalie_tendance  \n",
    "\n",
    "def _year(x):\n",
    "    return x.year + x.month / 12 + x.day / 365\n",
    "\n",
    "\n",
    "def _to_date_and_float(df):\n",
    "    pd.options.mode.chained_assignment = None \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "            except ValueError:\n",
    "                pass\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"datetime64[ns]\":\n",
    "            df[col] = df[col].apply(lambda x: _year(x))\n",
    "        elif df[col].dtype == \"int64\":\n",
    "            df[col] = df[col].apply(lambda x: float(x))\n",
    "        else:\n",
    "            pass\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57230a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(data.data['saledate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de1e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e5ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ecb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l2 = pd.DataFrame({col: l2_cov[:, idx] for idx, col in enumerate(df_nb.columns)}, index=[col for col in df_nb.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469daa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f401a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (40, 40)\n",
    "plt.pcolor(df_l2, cmap='gray')\n",
    "plt.yticks(np.arange(0.5, len(df_l2.index), 1), df_l2.index)\n",
    "plt.xticks(np.arange(0.5, len(df_l2.columns), 1), df_l2.columns)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835ecc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2_cov = np.zeros((df_nb.columns.shape[0], df_nb.columns.shape[0]))\n",
    "for i, col1 in enumerate(df_nb.columns):\n",
    "    for j, col2 in enumerate(df_nb.columns):\n",
    "        l2_cov[i, j] = np.linalg.norm(np.array(df_nb[col1].values.tolist()) - np.array(df_nb[col2].values.tolist()))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af9179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit_transform(np.array(df_nb[col1].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74939dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(df_nb['UsageBand'].dropna().values.tolist()).mean(axis=1), 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910257ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usage_band_na = df_nb['UsageBand']\n",
    "df_usage_band_na['true_index'] = df_nb.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_idx = df_usage_band_na.dropna().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f6c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db967eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.array(df_nb['UsageBand'].dropna().values.tolist()).mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d456337",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'UsageBand'\n",
    "df_usage_band_na = df_nb[col]\n",
    "df_usage_band_na['true_index'] = df_nb.index.values.tolist()\n",
    "true_idx = df_usage_band_na.dropna().index\n",
    "col_name = 0\n",
    "dff = pd.DataFrame(np.array(df_nb[col].dropna().values.tolist()).mean(axis=1))\n",
    "mean_df, std_df=dff[col_name].mean(), dff[col_name].std()\n",
    "ind = _z_score(col=dff[col_name], mean=mean_df, std=std_df, tresh=3)\n",
    "df2.loc[true_idx[dff[col_name].loc[ind].index.values.tolist()]][col]\n",
    "#pd.DataFrame((dff[[\"Unnamed: 0\", col_name]]).loc[ind].dropna(axis=0)).shape\n",
    "#pd.DataFrame((dff[[\"Unnamed: 0\", col_name]]).loc[ind].dropna(axis=0)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ind))\n",
    "print(mean_df, std_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e5b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(dff[ col_name], 'b.', label=\"all\")\n",
    "plt.plot(dff[ col_name].loc[ind], 'r*', label=\"outlier\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[true_idx[dff[ col_name].loc[ind].index.values.tolist()]]['UsageBand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e637e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df2['fiModelDesc'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _z_score(col, mean, std, tresh=6):\n",
    "    \"\"\"cutting distribution between mean-6*std and mean+6*std\n",
    "    Args:\n",
    "        df ([type]): [description]\n",
    "        col_name ([type]): [description]\n",
    "        mean ([type]): [description]\n",
    "        std ([type]): [description]\n",
    "        tresh (int, optional): [description]. Defaults to 6.\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    upper_bound = mean + tresh * std\n",
    "    lower_bound = mean - tresh * std\n",
    "    idx = col[\n",
    "        ~((col > lower_bound) & (col < upper_bound))\n",
    "    ].index  # trancate values from the column\n",
    "    print(mean, std)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marque = pd.read_csv(r'improved_data_quality\\data\\echantillon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marque_nb = _string_to_nbr(df_marque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dfbbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marque_nb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afe40b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708f550",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col = 'MARQUE'\n",
    "df_usage_band_na = df_marque_nb[col]\n",
    "df_usage_band_na['true_index'] = df_nb.index.values.tolist()\n",
    "true_idx = df_usage_band_na.dropna().index\n",
    "col_name = 0\n",
    "dff = pd.DataFrame(np.array(df_marque_nb[col].dropna().values.tolist()).mean(axis=1))\n",
    "mean_df, std_df=dff[col_name].mean(), dff[col_name].std()\n",
    "ind = _z_score(col=dff[col_name], mean=mean_df, std=std_df, tresh=6)\n",
    "df_marque.loc[true_idx[dff[col_name].loc[ind].index.values.tolist()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(dff[ col_name], 'b.', label=\"all\")\n",
    "plt.plot(dff[ col_name].loc[ind], 'r*', label=\"outlier\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
