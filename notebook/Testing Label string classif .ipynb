{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "%load_ext autotime\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import distance\n",
    "import random\n",
    "from itertools import repeat \n",
    "from sklearn.cluster import DBSCAN\n",
    "from numpy import nan\n",
    "import jellyfish\n",
    "from scipy.spatial import distance\n",
    "import utils\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import linalg, eye, csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "class MarkovClustering:\n",
    "    def __init__(self, matrix, metric=\"cosine\", bias=1):\n",
    "        \"\"\"\n",
    "        Initializing similarity matrix\n",
    "        Either by setting it (metric = None)\n",
    "        Or by deriving it from a distance function (similarity = bias-distance)\n",
    "        \"\"\"\n",
    "        self.labels_ = None\n",
    "        if metric is None:\n",
    "            self.T = matrix\n",
    "        else:\n",
    "            self.T = csr_matrix(bias-pairwise_distances(matrix, metric=metric))\n",
    "\n",
    "    def normalize(self):\n",
    "        self.T = normalize(self.T, norm='l1', axis=1)\n",
    "\n",
    "    def self_loops(self, weight=0.01):\n",
    "        self.T = eye(self.T.shape[0]) * weight + self.T\n",
    "\n",
    "    def expansion(self, p=2):\n",
    "        ret = self.T\n",
    "        for _ in range(1,p):\n",
    "            ret = ret * self.T\n",
    "        self.T = ret\n",
    "\n",
    "    def inflation(self, p=2, th=1e-10):\n",
    "        for i in range(len(self.T.data)):\n",
    "            if self.T.data[i] < th:\n",
    "                self.T.data[i] = 0\n",
    "            else:\n",
    "                self.T.data[i] = self.T.data[i] ** p\n",
    "\n",
    "    def fit(self, inflation_power=2, inflation_threshold=1e-10, self_loops_weight=0.01, expansion_power=2, iteration_limit=100,verbose=False):\n",
    "        iterations = 0\n",
    "        prev_T = csr_matrix(self.T.shape)\n",
    "        self.self_loops(self_loops_weight)\n",
    "        while (iterations < iteration_limit) and ((prev_T - self.T).nnz != 0):\n",
    "            prev_T = self.T\n",
    "            iterations += 1\n",
    "            self.normalize()\n",
    "            self.expansion(expansion_power)\n",
    "            self.inflation(inflation_power, inflation_threshold)\n",
    "            if verbose:\n",
    "                print (\"========Iteration #{i}=======\".format(i=iterations))\n",
    "                print(self.T.toarray())\n",
    "        self.labels_ = self.extract_labels()\n",
    "        return self\n",
    "\n",
    "    def extract_labels(self):\n",
    "        M = self.T.tocoo()\n",
    "        rows = defaultdict(set)\n",
    "        for i, d in enumerate(M.data):\n",
    "            if d == 0:\n",
    "                continue\n",
    "            rows[M.row[i]].add(M.col[i])\n",
    "        hash_row = lambda l: \",\".join(map(str,sorted(l)))\n",
    "        row_hashes = [hash_row(rows[i]) for i in range(M.shape[0])]\n",
    "        d = dict([(l,i) for i,l in enumerate(set(row_hashes))])\n",
    "        labels = [d[row_hashes[i]] for i in range(M.shape[0])]\n",
    "        return labels\n",
    "\n",
    "    def clusters(self, labels=None):\n",
    "        ret = defaultdict(set)\n",
    "        for i,c in enumerate(self.labels_):\n",
    "            if labels is None:\n",
    "                ret[c].add(i)\n",
    "            else:\n",
    "                ret[c].add(labels[i])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_file = \"~/Downloads/echantillon.csv\"\n",
    "\n",
    "df = pd.read_csv(Data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = np.asarray(df[\"MARQUE\"])\n",
    "words = np.asarray(df_names)\n",
    "df_unique = np.unique(df_names)\n",
    "unique_words = np.asarray(df_unique)\n",
    "X = CountVectorizer(max_df=10**-2, min_df=10**-7).fit_transform(unique_words)\n",
    "X = TfidfTransformer(use_idf=False).fit_transform(X)\n",
    "T = MarkovClustering(X)\n",
    "list_cluster = list(T.fit().clusters().items())\n",
    "# list_uncorrect = []\n",
    "# list_suggestion = []\n",
    "index_incorrect_words = []\n",
    "correct_words = []\n",
    "i = 0\n",
    "if len(list_cluster) == len(words):\n",
    "    print(\"all is unique\")\n",
    "else:\n",
    "    for cluster in list_cluster:\n",
    "#         print(cluster)\n",
    "        list_clust = list(cluster)[1]\n",
    "        cluster_count = []\n",
    "        if len(unique_words[list(list_clust)]) > 1 :\n",
    "#             print(unique_words[list(list_clust)])\n",
    "            _ , unique_counts = np.unique(df_names, return_counts=True)                        \n",
    "            count_cluster = [unique_counts[np.where(unique_words == w)[0]][0] for w in unique_words[list(list_clust)]]\n",
    "            for w in unique_words[list(list_clust)]:\n",
    "                count_w = unique_counts[np.where(unique_words == w)[0]][0]\n",
    "                if count_w < max(count_cluster):\n",
    "                    index_incorrect_words += np.ndarray.tolist(np.where(words == w)[0])  \n",
    "                    correct_words += [unique_words[list(list_clust)][np.argmax(count_cluster)]]*len(np.ndarray.tolist(np.where(words == w)[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[index_incorrect_words].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correct_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_file = \"~/Downloads/echantillon.csv\"\n",
    "\n",
    "df = pd.read_csv(Data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
