%this will contains the latex report
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage[allcolors=blue]{hyperref}
\usepackage{cleveref}
\usepackage{dirtytalk}

\newcommand\dhawat[1]{\textcolor{red}{DH: #1}}
\newcommand\mabaach[1]{\textcolor{magenta}{MA: #1}}
\newcommand\mboussaa[1]{\textcolor{blue}{MB: #1}}


\begin{document}
\section{Intoduction} % (fold)
\label{sec:Intoduction}
With any form of decision making, the quality of the decision is only as good as the data analysed, and so there is a need to ensure that the highest quality data is available for analysis. While having good levels of data quality improves analysis accuracy, having bad data quality can have a serious impact on the enterprise.
Poor data quality has a substantial impact, such as financial loss.  \href{https://www.ibm.com/blogs/journey-to-ai/}{IBM} estimate the loss of 3.1 trillion dollars annually  in USA due to poor data quality which is twice \href{https://data.worldbank.org/indicator/NY.GDP.MKTP.CD}{Canada’s GDP} in the very same year. Productivity loss, \href{https://www.forrester.com/report/Build-Trusted-Data-With-Data-Quality/RES83344}{Forrester} came up with a study that 40\% spend 1/3 of their time validating and fixing data quality issues. All of that lead to a low trust data.
Traditional data quality control methods are based on users’ experience or previously established business rules, and this
limits performance in addition to being a very time consuming process and low accuracy.

The main \mabaach{purpose} objective of this research \mabaach{is to examine the possible use of probability and machine learning techniques to .. ?} was to examine how can we use
probability models and machine learning to improve indicators
of data quality in a given dataset, with no \mabaach{without any prior knowledge of the domain ?} domain specific knowledge.
There was no particular regard to the domain of the dataset chosen, the main criteria for choice was the concept that bad data or outlier are rare data objects, \mabaach{\textit{i.e.}} i.e., those objects with rare combinations of feature values, compared to the majority of objects.
We built a package of Python algorithms to detect bad data present \mabaach{presence} in any type of table of data. \mabaach{Those} This algorithms are gathered in a Python class easy to use, and provided with detailed documentations, facilitating the comprehension and opening the door for further development of this class. The class contains test for many type of outlier (bad data) that could be found in a table of data such as: detection of repetition row, break of uniqueness rule, search for extreme values for columns with density distribution, non significant rows and columns, detection of spelling errors, and typographical error, outlier detection over rows, in each column and in each category of the columns with discreet distribution, detection of logical error between correlated columns.

This short survey is a report on the problem of data quality proposed by the company \href{ https://www.foyer.lu/en/homepage}{Foyer} during the challenges \href{https://challenge-maths.sciencesconf.org/}{\textit{mathematiques et entreprises}} organized by AMIES,  SFdS, SMF and SMAI.
In section \ref{sec:Data quality} we define \mabaach{what data quality is and different metrics used to asses it} the metrics describing the quality of data present in a table of data, and the difficulty arising while trying to find bad data, or to estimate the quality of the table in general. Then in section \ref{sec:Starategy used to detect bad data} we details our strategy used to deal with bad data. This section present theoretical mathematical concept behind the tests present in our Python package. In section \ref{sec:Algorithm}, we present some of the principal code used in our package described in section ref{sec:Starategy used to detect bad data}. In section \ref{sec:Experiments} we present the results of the test of our code on the table of data provides by the company \href{ https://www.foyer.lu/en/homepage}{Foyer} to test the efficiency of our methods. Finally, in section \ref{sec:Discussions}, we present the limitations of our algorithm and we propose solutions to overcome this limitations, and develope further the package.

% sectionIntoduction (end)

\section{Data quality} % (fold)
\label{sec:Data quality}
Let us first start with the heart of the problem, what can we consider as being good data? In an idyllic world, when we proceed to do data collection, there would be no duplication rows in the database, no missing values, no typos or illogical errors, in a sense, no human alteration of the picture that we are trying to capture from the real world. In classification issues more specifically, the data would be homogeneous and all classes would be equally represented. But of course that is not exactly what happens. \\

When speaking about data quality, several dimensions come to mind, as in \cite{amazon}, we could refer to the \textit{extension} of the data thus, focusing our attention on data values, or to the \textit{intension} of the data which represents the logical view of the database, the two dimensions are interconnected, the later is harder to measure, but highly influences the applicability of the extension's range of attributes.\\
In order to measure the quality of a database, one must consider several dimensions both regarding the extention and the intension side of the data, thus the quality of data could be related to different metrics such as the first and most obvious one that comes to mind would be \textit{redundancy} which refers to duplicated instances (\textit{i.e.} repeated rows or columns), then comes \textit{completeness} a.k.a. the comprehensiveness or wholeness of the data, it can be measured by the presence of null values (\textit{i.e.} or missing data), however it is important to consider it within the right context \say{indeed, in the schema of product category, the absence of a value for the attribute \texttt{shoe\_size} is not relevant for a product in the category \textit{notebooks}} \cite{amazon}, thus measuring the completeness is only relevant when the attribute is applicable. \\
\textit{Consistency} refers to whether the same data kept at different places do or do not match. \textit{Intra-relation constraints} define the range of admissible values within a column (\textit{i.e}, a specific data type, an interval for numerical columns, or set of values for categorical ones.), \textit{Inter-relation constraints} represents the logical inter-connectivity and interrelationship of the columns, then comes \textit{Accuracy}, which refers to whether the data values stored for an object are the ``correct values''. To be correct, a data values must be the right value and must be represented in a consistent and unambiguous form. there are two characteristics of accuracy: form and content. Form eliminates ambiguities about the content (\textit{i.e} \texttt{BIRTH\_DAY} would have different format depending on which representation we choose, USA or European), and content accuracy compares the value of a column with its real world representation (\textit{i.e} both 07/09/1993 and 09/07/1993 are valid in the context of \texttt{BIRTH\_DAY}), we can also mention other dimensions of data quality like the \textit{amount of data (imbalanced class), heterogeneity, reliability (for example a the year of birth is smaller than the year of the death), relevance} and \textit{timeliness}. \\
When dealing with data quality, we first need to assess the type of the data, distinguishing between \textit{categorical} and \textit{continuous} is vital, indeed the data type would highly influence the kind of errors we would be looking for (for nominal data one can check for typo errors (accuracy content dimension), for ordinal data the Intra-relation constraints would be important to acknowledge, for the continuous data, one would be looking for outliers using the data distribution). It would also influence the algorithmic techniques and tests that we would choose to apply. \\
Regarding outlier detection, the problem we are faced with is that in order for statistical techniques to work, we need to first assume that
\textit{almost surely}
the data is clean, thus the outlier behavior would be different enough that it would appear as an observation that was generated by a different mechanism as Hawkins officially defined it \cite{Hawking}.
Thus, it is necessary to combine deterministic approaches along with statistical ones to detect outliers, therefor we first need to ``clean'' the data by performing a serie of deterministic tests, identifying data behavior that ``obviously'' .
deviates from the ``norm'' for example if 99.99\% of the values of \texttt{column\_a} are higher than the values of \texttt{column\_b} it would mean that the remaining 0.01\% might be errors, which lead us to the section where we present the different strategies that we adopted to assess data quality.
\section{Starategy used to detect bad data}
\label{sec:Starategy used to detect bad data}



\subsection{Duplication test diala} % (fold)
\label{sub:Duplication test}



% subsectionDuplication test (end)


\subsection{Typographic test} % (fold)
\label{sub:Typographic test}
Detecting typo errors in strings is a tricky matter that relies on many factors such as how do we define similarities between strings. 
\subsubsection{Affinity propagation algorithm}
The first method we explored is clustering the words using a similarity matrix, it is a direct approach of the problem, it consists on first computing a similarity matrix between all the strings of the database, and then clustering them using a clustering algorithm that would not require the number of clusters as a parameter. After experimenting with many distances, we opted for the method \texttt{ratio} of the class \texttt{SequenceMatcher} from the python module\texttt{difflib} which showed the best results, this method return a measure of the sequences similarity, where $T$ is the total number of elements in both sequences, and $M$ is the number of matches, the method returns $\cfrac{2*M}{T}$ the downside of the method would be that it is computationally expensive, although it is possible to use the function \texttt{real\_quick\_ratio\(\)} which returns an upper bound on \texttt{ratio()} 
but this will impact the effectiveness of the method. Once we computed the similarity matrix, we use a method \texttt{AffinityPropagation} of the class \texttt{cluster} of the library \texttt{sklearn} \cite{sklearn}, which is to cluster the data, which is unsupervised machine learning algorithm that does not require the user to specify the number of clusters, based on the concept of ``message passing'' between data points, they can be seeing as a network where all data send messages to all other points. The subject of these messages are the willingness of the points being exemplars. Exemplars are points that explain the other data points ‘best’ and are the most significant of their cluster \cite{richi}. After the clustering, we would get count the number of occurences in each element of the cluster and decide that all elements that appear below a fixed threshold to be misspelling errors. The problem with the technique relies on the fact that it is hard to pick the ``right'' distance between the strings, one possibility would be to run the algorithm in each cluster after computing the similarity matrix of the cluster doing a second pass of the algorithm, empirical tests shows that it seems to work better, but further investigation are necessary.
The second method we proposed is based on a Markov clustering algorithm.
\subsubsection{Markov clustering}
The first step is to transform the data into a suitable format by doing feature extraction using \texttt{CountVectorizer} method from  \linebreak \texttt{sklearn.feature\_extraction.text} class, \cite{vector} which convert a collection of text documents to a matrix of token counts (\textit{i.e.} a token is an object that holds the place of something else that is usually larger or more complex.), Then we proceed and initialize an object of the \texttt{MarkovClustering} class \cite{MarkovClustering} and use it to perform a type of graph clustering using \textbf{Markov Chain theory} by first creating a \textbf{probability matrix of transition} then simulating a random walk by alternating between two operations \textbf{inflation} that would change the probability of transition to advantage strong neighbors and disadvantage weak ones and \textbf{expansion} that would connect different regions of the graph, it would repeat this process until the state of equilibrium is reached and the algorithm converges \cite{explainMCL}, after that we get the clustered data and apply the same technique as before to detect the misspelled words.\\
The problem with this technique is that the accumulation of hidden layers makes it difficult to assess why it works extremely well in some cases and it does not in others.
\subsection{Extreme values :diala} % (fold)
\label{sub:Extreme values}

% subsection Extreme values (end)

\subsection{Completeness: diala} % (fold)
\label{sub:Completeness}

% subsectionCompleteness (end)

\subsection{Total order relation: mariem} % (fold)
\label{sub:Total order relation}
The idea behind this algorithm is to find ordering errors between two numerical columns by detecting if there is an inherent order between them and flagging elements that does not respect it. For example all the values of the column \texttt{BIRTH\_YEAR} should be lower than there corresponding value in the column \texttt{DEATH\_YEAR}. We proceed to a tendency test between all numerical columns, let \texttt{column\_a} and \texttt{column\_b} be two numerical columns, we compute the ratio between the number of elements in \texttt{column\_a} that are lower to there corresponding in \texttt{column\_b} (row-wise comparison) and the length of the columns. We start by dropping the missing values first to avoid any problems. if the ratio is high that would mean that there is a hidden tendency between those two columns and that the values that does not respect that order might be outliers

\subsection{Outlier detection with machine learning diala} % (fold)
\label{sub:Outlier detection with machine learning}

% subsection  Outlier detection with machine learning (end)
\subsection{Frequency for logical errors diala} % (fold)
\label{sub:Frequency for logical errors}

% subsection Frequency for logical errors (end)

\subsection{Intra categorical extreme values mariem} % (fold)
\label{sub:Intra categorical extreme values}

% subsection Intra categorical extreme values (end)

% section  Starategy used to detect bad data (end)

\section{Algorithm medi} % (fold)
\label{sec:Algorithm}



% sectionAlgorithm (end)

\section{Experiments} % (fold)
\label{sec:Experiments}



% sectionExperiments (end)

\section{Discussions} % (fold)
\label{sec:Discussions}



% section Discussions (end)

\section*{Acknowledgments} % (fold)
\label{sec:Acknowledgments}



% sectionAcknowledgments (end)

\begin{thebibliography}{999}

    \bibitem{pan_cos_chen}
    Guansong Pang, Longbing Cao and Ling Chen:
    \emph{Outlier Detection in Complex Categorical Data
        by Modelling the Feature Value Couplings.}

    \bibitem{amazon}
    Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Celikel, Felix Biessmann:
    \emph{Automating LargeScale Data Quality Verification}
    \bibitem{Hawking}
    Simon HawkinsHongxing, HeGraham Williams, Rohan Baxte:
    \emph{Outlier Detection Using Replicator Neural Networks}
    \bibitem{richi}
    Ritchie Vink:
    \emph{Algorithm Breakdown: Affinity Propagation}
    \url{https://www.ritchievink.com/blog/2018/05/18/algorithm-breakdown-affinity-propagation/}
    \bibitem{sklearn}
    \url{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html}

    \bibitem{vector}
    \url{https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html}

    \bibitem{MarkovClustering}
    \bibitem{https://gist.github.com/urigoren/1f76567f3af56ed8c33f076537768a60}

    \bibitem{explainMCL}
    \url{https://medium.com/analytics-vidhya/demystifying-markov-clustering-aeb6cdabbfc7}
\end{thebibliography}
\end{document}
