%this will contains the latex report
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage[allcolors=blue]{hyperref}
\usepackage{cleveref}
\usepackage{dirtytalk}

\newcommand\dhawat[1]{\textcolor{red}{DH: #1}}
\newcommand\mabaach[1]{\textcolor{magenta}{MA: #1}}
\newcommand\mboussaa[1]{\textcolor{blue}{MB: #1}}


\begin{document}
\section{Intoduction} % (fold)
\label{sec:Intoduction}
With any form of decision making, the quality of the decision is only as good as the data analysed, and so there is a need to ensure that the highest quality data is available for analysis. While having good levels of data quality improves analysis accuracy, having bad data quality can have a serious impact on the enterprise.
Poor data quality has a substantial impact, such as financial loss.  \href{https://www.ibm.com/blogs/journey-to-ai/}{IBM} estimate the loss of 3.1 trillion dollars annually  in USA due to poor data quality which is twice \href{https://data.worldbank.org/indicator/NY.GDP.MKTP.CD}{Canada’s GDP} in the very same year. Productivity loss, \href{https://www.forrester.com/report/Build-Trusted-Data-With-Data-Quality/RES83344}{Forrester} came up with a study that 40\% spend 1/3 of their time validating and fixing data quality issues. All of that lead to a low trust data.
Traditional data quality control methods are based on users’ experience or previously established business rules, and this
limits performance in addition to being a very time consuming process and low accuracy.

The main objective of this research was to examine how can we use
probability models and machine learning to improve indicators
of data quality in a given dataset, with no domain specific knowledge.
There was no particular regard to the domain of the dataset chosen, the main criteria for choice was the concept that bad data or outlier are rare data objects, i.e., those objects with rare combinations of feature values, compared to the majority of objects.
We built a package of Python algorithms to detect bad data present in any type of table of data. This algorithms are gathered in a Python class easy to use, and provided with detailed documentations, facilitating the comprehension and opening the door for further development of this class. The class contains test for many type of outlier (bad data) that could be found in a table of data such as: detection of repetition row, break of uniqueness rule, search for extreme values for columns with density distribution, non significant rows and columns, detection of spelling errors, and typographical error, outlier detection over rows, in each column and in each category of the columns with discreet distribution, detection of logical error between correlated columns.

This short survey is a report on the problem of data quality proposed by the company \href{ https://www.foyer.lu/en/homepage}{Foyer} during the challenges \href{https://challenge-maths.sciencesconf.org/}{\textit{mathematiques et entreprises}} organized by AMIES,  SFdS, SMF and SMAI.
In section \ref{sec:Data quality} we define the metrics describing the quality of data present in a table of data, and the difficulty arising while trying to find bad data, or to estimate the quality of the table in general. Then in section \ref{sec:Starategy used to detect bad data} we details our strategy used to deal with bad data. This section present theoretical mathematical concept behind the tests present in our Python package. In section \ref{sec:Algorithm}, we present some of the principal code used in our package described in section ref{sec:Starategy used to detect bad data}. In section \ref{sec:Experiments} we present the results of the test of our code on the table of data provides by the company \href{ https://www.foyer.lu/en/homepage}{Foyer} to test the efficiency of our methods. Finally, in section \ref{sec:Discussions}, we present the limitations of our algorithm and we propose solutions to overcome this limitations, and develope further the package.



% sectionIntoduction (end)

\section{Data quality} % (fold)
\label{sec:Data quality}
Let us first start with the heart of the problem, what can we consider as being good data? In an idyllic world, when we proceed to do data collection, there would be no duplication in the database, no missing values, no typos or illogical errors, in a sense, no human alteration of the picture that we are trying to capture from the real world. The data would be homogeneous and all classes would be equally represented. But of course that is not exactly what happens. \\
When speaking about data quality, several dimensions come to mind, as in \cite{amazone}, we could refer to the \textit{extension} of the data thus, focusing our attention on data values, or to the \textit{intension} of the data which represents the logical view of the database, the two dimensions are interconnected, the later is harder to measure, but highly influences the applicability of the extension's range of attributes.\\ 
In order to measure the quality of a database, one must consider several dimensions both regarding the extention and the intension side of the data, the first and most obvious one that comes to mind would be \textit{redundancy} which refers to duplicated instances (\textit{i.e.} repeated rows or columns), then comes \textit{completeness} a.k.a. the comprehensiveness or wholeness of the data, it can be measured by the presence of null values (\textit{i.e.} or missing data), however it is important to consider it within the right context \say{indeed, in the schema of product category, the absence of a value for the attribute \texttt{shoe\_size} is not relevant for a product in the category \textit{notebooks}} \cite{amazone}, thus measuring the completeness is only relevant when the attribute is applicable. \\
\textit{Consistency} refers to whether the same data kept at different places do or do not match. \textit{Intra-relation constraints} define the range of admissible values within a column (\textit{i.e}, a specific data type, an interval for numerical columns, or set of values for categorical ones.), \textit{Inter-relation constraints} represents the logical inter-connectivity and interrelationship of the columns, then comes \textit{Accuracy}, which refers to whether the data values stored for an object are the ``correct values''. To be correct, a data values must be the right value and must be represented in a consistent and unambiguous form. there are two characteristics of accuracy: form and content. Form eliminates ambiguities about the content (\textit{i.e} \texttt{BIRTH\_DAY} would have different format depending on which representation we choose, USA or European), and content accuracy compares the value of a column with its real world representation (\textit{i.e} both 07/09/1993 and 09/07/1993 are valid in the context of \texttt{BIRTH\_DAY}), we can also mention other dimensions of data quality like the \textit{amount of data (imbalanced class), heterogeneity, reliability (for example a the year of birth is smaller than the year of the death), relevance} and \textit{timeliness}. \\
When dealing with data quality, we first need to assess the type of the data, distinguishing between \textit{categorical} and \textit{continuous} is vital, indeed the data type would highly influence the kind of errors we would be looking for (for nominal data one can check for typo errors (accuracy content dimension), for ordinal data the Intra-relation constraints would be important to acknowledge, for the continuous data, one would be looking for outliers using the data distribution). It would also influence the algorithmic techniques and tests that we would choose to apply. \\

Regarding outlier detection, the problem we are faced with is that in order for statistical techniques to work, we need to first assume that \textit{almost surely} the data is clean, thus the outlier behavior would be different enough that it would appear as an observation that was generated by a different mechanism as Hawkins officially defined it. Thus, it is necessary to combine deterministic approaches along with statistical ones to detect outliers, therefor we first need to ``clean'' the data by performing a serie of deterministic tests, identifying data behavior that ``obviously'' deviates from the ``norm'' for example if 99.99\% of the values of \texttt{column_a} are higher than the values of \texttt{column_b} it would mean that the remaining 0.01\% might be errors.\\



% section  Data quality (end)

\section{Starategy used to detect bad data} % (fold)
\label{sec:Starategy used to detect bad data}



% section  Starategy used to detect bad data (end)

\section{Algorithm} % (fold)
\label{sec:Algorithm}



% sectionAlgorithm (end)

\section{Experiments} % (fold)
\label{sec:Experiments}



% sectionExperiments (end)

\section{Discussions} % (fold)
\label{sec:Discussions}



% section Discussions (end)

\section*{Acknowledgments} % (fold)
\label{sec:Acknowledgments}



% sectionAcknowledgments (end)

\begin{thebibliography}{999}

    \bibitem{pan_cos_chen}
    Guansong Pang, Longbing Cao and Ling Chen:
    \emph{Outlier Detection in Complex Categorical Data
        by Modelling the Feature Value Couplings.}


\end{thebibliography}
\end{document}
