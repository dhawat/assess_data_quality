%this will contains the latex report
\documentclass{article}
\usepackage[utf8]{inputenc}

\title{dsds}
\author{dialahawat7 }
\date{August 2021}

\begin{document}

\maketitle

\section{Introduction}

why is data quality important:
there are many reason for why data quality are important. First the is growth of data within a company among the years is at exponential rate. the next point is how interconnected are the systems in terms of how much data is following between your systems. the third is increasingly data driven organization.
In order to make sure that your theorem data quality is the best it can be we should follow this guide lines:
Poor data quality has a substantial impact, such as financial loss. IVM estimate the loss of 3.1 trillion dollars annually  in USA because due to poor data quality. Productivity loss, Forrester came up with a study that 40\% spend 1/3 of their time validating and fixing data quality issues. All of that lead to a low trust data.
Accuracy: is the data correct and precise? Create processes for verifying, entering, and updating CRM data right from the start.
Completeness: Do we have all the information needed for each of the records? Identify what information is important for what actions and develop processes for getting thr information your team needs to do their jobs
Uniformity: Is similar information presented in similar ways across your system?
Uniqueness : is yours CRM full of duplicate data? Have process to combine and delete duplicate records or to avoid creating them in the first place.
Timeliness: Is your information up to date? Regularly organize and clean your CRM data to ensure it's accurate unique and timely. Check for outdates information when perform bulk data imports.
security: Is your information secure? Give each user the access they need to do their jobs and nothing more.
\section{Approach to managing data quality }
Traditional approach to managing data quality is full of pain points. It's done by 3 steps. The first is the identification which consists on asses each dataset, meeting the owners and develop fixed rules to detect known issue, Then the remediation which consists on creating replacement rules where possible, and finally the recommendation consisting on alerts all given similar treatment and manual investigations to know what other rules should be qualify as bad data.
What can banking fraud teach us about how to fight data quality issues?
Use the technics like that used to fight fraud, to fight fraud data issues since data since bad data is fraudulante data record leaving whithin your data set.
Using AI technics applied to fight fraud for fighting data issue.
So that in the identification step, they use some anomaly detection using unsupervised ML, find data records that looks like outliers, or supervised machine learning or they imbed an active learning to identify data quality issues so that after identifying data issues. In the remediation step they used a supervised ML to recommend what should we fixe, use active learning and then use supervised ML to automaticaly recommend how should we fix them or RPA.
so the potential solution could look likes :
\begin{enumerate}
    \item identifying data quality issues: missing values, out of range, duplicates abd inconsistencies
    \item suggests and/or remediates the issues: depending on how certain the model is it is possible to take different actions
    \item Improves performance through active learning: giving feedback to the model the results will improve over time
\end{enumerate}
\section{unsupervised machine learning:}
%https://mobidev.biz/blog/unsupervised-machine-learning-improve-data-quality
\subsection{DIMENSIONALITY REDUCTION ALGORITHM} % (fold)
\label{sub:DIMENSIONALITY REDUCTION ALGORITHM}
Dimensionality reduction is helpful when working with visual and audio data involving speech, video, images, or text, and also when simplifying datasets in order to better fit a predictive model.
Dimensionality reduction is the ML method used to identify patterns in data and deal with computationally extensive problems. This method includes a set of algorithms aimed to reduce the number of input variables in a dataset by projecting the original high-dimensional input data to a low-dimensional space.
UMAP (Uniform Manifold Approximation and Projection) algorithm allows the construction of a high dimensional graph representation of data and further optimization of a low-dimensional graph to be as structurally similar as possible.
DATA CLUSTERING:
This approach is applicable for market and customer segmentation, recommendation engines, document classification, fraud identification, and other cases.
Clustering allows defining a structure in a set of unlabeled data. Simply put, it organizes data into groups (clusters), basing on its similarity and dissimilarity.
DBSCAN (density-based spatial clustering of applications with noise) algorithm takes all instances that are close to each other and groups them together, based on a distance measurement and a minimum number of instances specified by a data science engineer.
ANOMALY DETECTION ALGORITHM
The anomaly detection algorithm is not independent itself. It often goes along with the dimensionality reduction and clustering algorithms.By using the dimensionality reduction algorithm as a pre-stage for anomaly detection, we, firstly, transform high-dimensional space into a lower-dimensional one. Then we can figure out the density of the major data points in this lower-dimensional space, which may be identified as “normal.” Those data points located far away from the “normal” space are outliers or “anomalies.”
ASSOCIATION MINING ALGORITHM
Association mining is the unsupervised ML algorithm used to identify hidden relationships in large datasets which frequently occur together. association mining can deal with non-numeric, categorical data, which means that it requires more actions than simple counting.
In the public sector, the association mining algorithm can be used for census data processing. As a result, it may help to optimize the planning of public services and businesses such as transport, education, health, setting up new shopping malls, and factories.
we did the following:

Created the virtual machine on the cloud
Benchmarked it via running different tests in order to measure the performance of VM
Collected about 2000 characteristics as raw data
Analyzed the gathered raw data and extracted the most valuable benchmarks
Compressed benchmarks into several coefficients (parallelization, one core, stability, database, RAM)
Calculated the custom coefficient as a balance between the performance and price
Selected the best instance type based on their characteristics and price
In conclusion, I would say that there is no perfect path for any business case. And unsupervised machine learning is just a tool for getting expected results. It will work well if you are sure that it meets your business requirements.
\section{Improving Data Quality Through Deep Learning and Statistical Models:}
traditional data quality control
methods are based on users’ experience or
previously established business rules, and this
limits performance in addition to being a very
time consuming process and low accuracy.
Utilizing big data, we can leverage computing
resources and advanced techniques to overcome
these challenges and provide greater value to the
business.
Machine learning is a promising subfield of
computer science to approach human-level
artificial intelligence (AI), and is already used to
self-driving car, data mining, and natural speech
recognition.
The AI algorithms can either
be unsupervised or supervised. Additionally,
deep learning is one of the supervised machine
learning algorithms.
Improving data quality
is essential to data mining, data analysis, and big
data. [16] mentions data quality assessment
architecture via AI, but this paper does not
discuss how to improve and how to identify data
quality with special details based on AI and
statistical modes.
To the best of our knowledge,
no literature improves data quality through bot
machine learning and statistical quality control
models
\subsection{outlier def}
Outlier data means this data is totally
different from the others. Hawkins officially
defined it as “An outlier is an observation which
deviates so much from the other observations as
to arouse suspicions that it was generated by a
different mechanism”.
However, outlier data
does not mean error data or problem data, but
this data means it has potential error data or risk
dataß
\subsection{work in the paper}
In this paper, we develop deep learning
software based on KNIME and WEKA [16-20]
because this software are visually interactive
tools and the software supports JAVA programs.
% subsection DIMENSIONALITY REDUCTION ALGORITHM (end)
\end{document}
