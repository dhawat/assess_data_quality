this will contains the latex report
\begin{document}
\section{introduction}
why is data quality important:
there are many reason for why data quality are important. First the is growth of data within a company among the years is at exponential rate. the next point is how interconnected are the systems in terms of how much data is following between your systems. the third is increasingly data driven organization.
In order to make sure that your theorem data quality is the best it can be we should follow this guide lines:
Poor data quality has a substantial impact, such as financial loss. IVM estimate the loss of 3.1 trillion dollars annually  in USA because due to poor data quality. Productivity loss, Forrester came up with a study that 40\% spend 1/3 of their time validating and fixing data quality issues. All of that lead to a low trust data.
Accuracy: is the data correct and precise? Create processes for verifying, entering, and updating CRM data right from the start.
Completeness: Do we have all the information needed for each of the records? Identify what information is important for what actions and develop processes for getting thr information your team needs to do their jobs
Uniformity: Is similar information presented in similar ways across your system?
Uniqueness : is yours CRM full of duplicate data? Have process to combine and delete duplicate records or to avoid creating them in the first place.
Timeliness: Is your information up to date? Regularly organize and clean your CRM data to ensure it's accurate unique and timely. Check for outdates information when perform bulk data imports.
security: Is your information secure? Give each user the access they need to do their jobs and nothing more.
\section{Approach to managing data quality }
Traditional approach to managing data quality is full of pain points. It's done by 3 steps. The first is the identification which consists on asses each dataset, meeting the owners and develop fixed rules to detect known issue, Then the remediation which consists on creating replacement rules where possible, and finally the recommendation consisting on alerts all given similar treatment and manual investigations to know what other rules should be qualify as bad data.
What can banking fraud teach us about how to fight data quality issues?
Use the technics like that used to fight fraud, to fight fraud data issues since data since bad data is fraudulante data record leaving whithin your data set.
Using AI technics applied to fight fraud for fighting data issue.
So that in the identification step, they use some anomaly detection using unsupervised ML, find data records that looks like outliers, or supervised machine learning or they imbed an active learning to identify data quality issues so that after identifying data issues. In the remediation step they used a supervised ML to recommend what should we fixe, use active learning and then use supervised ML to automaticaly recommend how should we fix them or RPA.
so the potential solution could look likes :
\begin{enumerate}
    \item identifying data quality issues: missing values, out of range, duplicates abd inconsistencies
    \item suggests and/or remediates the issues: depending on how certain the model is it is possible to take different actions
    \item Improves performance through active learning: giving feedback to the model the results will improve over time
\end{enumerate}
\end{document}
