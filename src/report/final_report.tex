%this will contains the latex report
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage[allcolors=blue]{hyperref}
\usepackage{cleveref}
\usepackage{dirtytalk}
\usepackage{graphicx}
\usepackage{float} %fixe image position
\usepackage[ruled,vlined]{algorithm2e}
% hyperlink
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    }
\title{\Large Report for AMIES challenge, Wasserstein-blind group:\ Automatic data quality assessment}

\author{Abaach Mariem, Boussaa Mehdi, Hawat Diala}
\usepackage{fancyhdr}
%\pagestyle{fancy}
%\makeatletter
%\let\runauthor\@author
%\let\runtitle\@title
%\makeatother


\date{}

\newcommand\dhawat[1]{\textcolor{red}{DH: #1}}
\newcommand\mabaach[1]{\textcolor{magenta}{MA: #1}}
\newcommand\mboussaa[1]{\textcolor{blue}{MB: #1}}


\begin{document}
\maketitle

\begin{abstract}
    Data is what guides today's decision making process, everyone relies on it, it is at the center of modern institutions, but according to the saying : GIGO (Garbage In Garbage Out), bad data may have detrimental consequences on the company that used it.  It is then of crucial order that the data be of best quality possible, however, the process of cleaning the data usually relies on deterministic rules, which makes it hard, tedious and time consuming. Thus AMIES along with the company Foyer proposed a challenge about the automation of the process. This problem required us to first assess what we meant by ``good data quality'' which led to a definition of data quality and to proposed algorithms that need as little as possible of human intervention.
\end{abstract}
\tableofcontents
\section{Intoduction} % (fold)
\label{sec:Intoduction}
With any form of decision making, the quality of the decision is only as good as the data analysed, and so there is a need to ensure that the highest quality data is available for analysis. While having good levels of data quality improves analysis accuracy, having bad data quality can have a serious impact on the enterprise.
Poor data quality has a substantial impact, such as financial loss.  \href{https://www.ibm.com/blogs/journey-to-ai/}{IBM} estimate the loss of 3.1 trillion dollars annually  in the USA due to poor data quality which is twice \href{https://data.worldbank.org/indicator/NY.GDP.MKTP.CD}{Canada’s GDP} in the very same year. Productivity loss, \href{https://www.forrester.com/report/Build-Trusted-Data-With-Data-Quality/RES83344}{Forrester} came up with a study that 40\% of data scientists spend 1/3 of their time validating and fixing data quality issues. All of that lead to a low trust in data.
Traditional data quality control methods are based on users’ experience or previously established business rules, and this
limits performance in addition to being a very time consuming process and low accuracy.

The main purpose of this research is to examine the possible use of probability and machine learning techniques to improve indicators
of data quality in a given dataset, with no domain specific knowledge.
There was no particular regard to the domain of the dataset chosen, the main criteria for choice was the concept that bad data or outlier are rare data objects, \textit{i.e.} , those objects with rare combinations of feature and values, compared to the majority of objects.
We built a package of Python algorithms to detect bad data present in any type of table of data. Those algorithms are gathered in a Python class easy to use, and provided with detailed documentations, facilitating the comprehension and opening the door for further development of this class. The class contains test for many type of outlier (bad data) that could be found in a table of data such as: detection of repetition row, break of uniqueness rule, search for extreme values for columns with density distribution, non significant rows and columns, detection of spelling errors, and typographical error, outlier detection over rows, in each column and in each category of the columns with discrete distribution, detection of logical error between correlated columns.

This short survey is a report on the problem of data quality proposed by the company \href{ https://www.foyer.lu/en/homepage}{Foyer} during the challenge \href{https://challenge-maths.sciencesconf.org/}{\textit{mathematiques et entreprises}} organized by AMIES,  SFdS, SMF and SMAI.
In section \ref{sec:Data quality} what data quality is and different metrics used to asses it, and the difficulty arising while trying to find bad data, or to estimate the quality of the table in general. Then in section \ref{sec:Starategy used to detect bad data} we detail our strategy used to deal with bad data. This section present theoretical mathematical concept behind the tests present in our Python package. In section \ref{sec:Algorithm}, we present some of the principal code used in our package described in section ref{sec:Starategy used to detect bad data}. In section \ref{sec:Experiments} we present the results of the test of our code on the table of data provides by the company \href{ https://www.foyer.lu/en/homepage}{Foyer} to test the efficiency of our methods. Finally, in section \ref{sec:Discussions}, we present the limitations of our algorithm and we propose solutions to overcome this limitations, and develope further the package.

% sectionIntoduction (end)

\section{Data quality} % (fold)
\label{sec:Data quality}
Starting from the basic problem, what can we consider as being good data?
In an idyllic world, when we proceed to do data collection, we target data with no duplicated rows, no missing values, no typos or illogical errors, in a sense, no alteration of the picture that we are trying to capture from the real world.
More specifically, in classification issues, the target is homogeneous data where all classes are equally represented.
But of course, this is not what we obtain.
Hence, we refer to data quality as a quantification of the difference between our ideal expectations and the available result and we project these differences to several axes.
The trade-off is to consider two basic axes.
First, the \textit{extension} of the data focusing our attention on data values and second the \textit{intension} of the data representing the logical view of the database \cite{amazon}.
The two are interconnected.
The latter is harder to measure, but highly influences the applicability of the extension's range of attributes.
Regarding the extension and the intension axes, the quality of data is subject to different metrics.
The first and most obvious one is \textit{redundancy} which refers to duplicated instances i.e., repeated rows or columns.
Second \textit{completeness} i.e., the comprehensiveness or wholeness of the data, measured by the presence of missing data.
However, completeness should be considered in the right context.
For example, in the schema of product category, the absence of a value for an attribute \texttt{shoe\_size} is not relevant for a product in the category \texttt{notebooks} \cite{amazon}.
Thus measuring completeness is only relevant when the attribute is applicable.
\textit{Consistency} is the degree to which a set of semantic rules are violated.
It defines the range of admissible values within a column e.g., a specific data type, an interval for numerical columns, or a set of values for nominal ones.
It represents the logical inter-connectivity of the columns.
Finally, the \textit{Accuracy}, which refers to whether the values stored are the correct values.
More precisely, data values must be the right values and must be represented in a consistent and unambiguous form.
In other words, a data may be consistent but inaccurate.
For example, for a product attribute \texttt{color}, a value "blue" can be considered consistent even if the correct value would be "red", whereas a value "XL" would be considered neither consistent nor accurate.
% We can identify two characteristics of accuracy.
% The form and the content.
% The form eliminates ambiguities about the content.
% For example, \texttt{BIRTH\_DAY} would have different formats depending on which representation we choose, the USA or European, and content accuracy compares the value of a column with its real-world representation.
We can also mention other metrics like the \textit{amount of data (imbalanced class), heterogeneity, reliability, relevance}, and \textit{timeliness}.
Note that we need first to assess the type of the data, distinguishing between \textit{categorical} and \textit{continuous}.
The data type would highly influence the kind of errors we would be looking for and the algorithmic choice.
For example, for categorical data, one can check for typo errors (accuracy content dimension), for ordinal data the Intra-relation constraints would be important to acknowledge, and for continuous data one would be looking for outliers.
Regarding outlier detection, the major problem is that the validity of available statistical technics relies on assuming that the data is clean.
This is not the case while dealing with bad data.
Thus, in this framework outlier detection is a hard task highly dependent on the corrupted data missed to identify in the phase of cleaning the data.
\section{Starategy used to detect bad data}
\label{sec:Starategy used to detect bad data}
In this section, we describe the tests we used to identify bad data contained in the dataset provided by the company Foyer.
The corresponding pseudo codes differ to Section \ref{sec:Pseudo-code}.
%We combined all these tests and proposed a strategy to construct a plug-and-play Python Class denoted \texttt{Data}.
%The code is available upon request.

\subsection{Redundancy} % (fold)
\label{sub:Redundancy}
We start by identifying the duplicated rows.
Duplicated rows are bad data and can bias other tests.
They can alter the approximated distribution of numerical columns so the duplication test is the first deterministic test applied to the data.
We drop the duplicated rows before doing any other test.
For the corresponding pseudo-code see Appendix \ref{code:Duplicate errors algorithm}.
% subsectionDuplication test (end)

\subsection{Completeness} % (fold)
\label{sub:Completeness}
The completeness of data is an important metric to consider.
We construct a simple algorithm to flag rows with a high ratio of missing values as bad rows with error type incompleteness.
The corresponding pseudo-code defers to Appendix \ref{code:completeness}.
% subsectionCompleteness (end)

\subsection{Consistency} % (fold)
\label{sub:consistency}
In this section, we try to identify corrupted data concerning the consistency metric first for nominal categorical columns and second for numerical columns.
\subsubsection{For nominal categorical columns}
One of the major problems with our dataset is the presence of typographical errors in the nominal categorical columns.
A typographical error is any spelling error like a misspelled mark of cars.
Detecting this type of error is a hard task.
One should first define similarities between strings or encode strings in numerical values in an isometric way.
We have investigated two clustering methods that don't require the number of clusters to be determined or estimated before running the algorithm.
The first is an affinity propagation technic, and the last is a Markov clustering method.
The pseudo-codes defers to Appendix \ref{sub:Typographical error algorithm}.
\dhawat{add few words: why we use 2 methods, difference, efficacity of each}

\paragraph{ Affinity propagation.}
\label{sub:Affinity propagation algorithm}
The first method we used to detect typographical errors consists of partitioning each nominal categorical column into clusters using a similarity matrix.
Then assuming that the correct spelling of a word will have the highest frequency in the cluster, words with low frequencies are spelling errors.
For example, if a cluster contains "Single", "Singl", "Singel", and "SINgle" we expect that "Single" will have the highest frequency in the cluster.
We computed the similarity matrix using \texttt{SequenceMatcher} from the python package \href{https://docs.python.org/3/library/difflib.html}{difflib}.
After experimenting with many distances, we opted for \texttt{ratio()} which showed the best results.
This method returns a measure of similarity by the float $2M /T$ where, for any two words, $T$ is the total number of elements in both, and $M$ is the number of matches.
The downside of this method is that it is computationally expensive.
Although it is possible to use \texttt{real\_quick\_ratio()} to obtain quickly an upper bound on \texttt{ratio()}, it will impact the efficacity of the method.
Once we have the similarity matrix, we use the Class \texttt{AffinityPropagation} from the python package \href{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html#sklearn-cluster-affinitypropagation}{sklearn} to cluster the data.
It is an unsupervised machine learning algorithm that does not require the user to specify the number of clusters, unlike clustering algorithms such as k-means or k-medoids.
It consists of finding the elements (usually denoted by \texttt{exemplars}) that are the most representative of the clusters.
In other words, \texttt{exemplars} are the points that explain the other points \textbf{best} and are the most significant of their cluster.
After clustering, we count the frequency of each word in each cluster i.e., the number of occurrences over the total number of words in the cluster.
Then we fix a threshold and all words having a frequency less than this threshold are considered typographical errors.
\dhawat{Unclear from here}
The problem of this method is that it is hard to pick the ``right'' distance between the strings, one possibility would be to run the algorithm in each cluster after computing the similarity matrix of the cluster doing a second pass of the algorithm, empirical tests shows that it seems to work better, but further investigation are necessary.
\dhawat{until here, please check}

\paragraph{Markov clustering}
The second method we proposed is based on a Markov clustering machine learning algorithm.
Text data requires special preparation before you can apply a machine learning algorithm.
Each word needs to be represented by an integer or a floating-point value.
So, the first step is to transform a wordy column into a suitable format.
We used the Class \href{https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html}{\texttt{CountVectorizer}} from \texttt{sklearn}.
It converts a collection of text documents to a matrix of token counts.
A token is an object replacing a larger or more complex object.
Second, we use a graph-based Markov clustering method \footnote{An implementation of the algorithm is provided by \href{https://gist.github.com/urigoren/1f76567f3af56ed8c33f076537768a60}{https://gist.github.com/urigoren/}}, which also doesn't require the specification of the number of clusters.
We applied \href{https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html}{\texttt{TfidfTransformer}} to the matrix of token counts and we used the obtained matrix as the transition matrix of the Markov Chain.
The Markov clustering algorithm uses the transition matrix to initiate a random walk by alternating between two operations.
The first changes the transition probability privileging strong neighbors, and the last connect different regions of the graph.
The process will be repeated until reaching an equilibrium state.
We finally get the clustered data and apply the same technic used in the previous paragraph to identify misspelled words.
\dhawat{unclear: from}The problem with this technic is that the accumulation of hidden layers makes it difficult to assess why it works extremely well in some cases and it does not in others. \dhawat{till. clarify please in which case it works ...}
% subsectionTypographic test (end)

\subsubsection{For numerical columns} % (fold)
\label{ssub:extreme_value_test}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{picture/normal.png}
    \caption{Normal distribution.}
    \label{fig:normal distribution}
\end{figure}
We used the following extreme values test to identify inconsistent data in non-discrete numerical columns like negative dates, extreme high/low prices of cars...
The test consits on supposing that this type of column follows a Normal distribution.
A Normal distribution is a symmetric, continuous, and bell-shaped distribution.
For Normal distributions, within one standard deviation from the mean lie 68\% of the values, while within 3 standard deviations lie 99.73\%, and within 6 standard deviations lie 99,99\% of the values (Figure~\ref{fig:normal distribution}).
Hence, we suppose that non-constant and non-discrete (non-clustering) numerical columns follow a Normal distribution, and extreme values lie outside the mean plus/minus six standard deviations.
We denote this test by the extreme values test.
Although it successfully detects all the extreme values within our data with a sufficiently small ratio of false negatives, it may give weird results if the distribution of the column is not Normal.
We recommend starting with a test of Gaussianity before proceeding with the extreme values test.
The pseudo-code is deferred to Appendix \ref{code:extreme_values_numerical}.
% subsection Extreme values (end)

\subsection{Accuracy} % (fold)
\label{sub:Accuracy}
In this section, we try to identify corrupted data concerning the accuracy metric first for nominal categorical columns, second for numerical columns, and finally for combinations of both types.
We refer to bad data identified with the accuracy metric as outliers or logical errors.
Two major types of outliers may be present in a dataset, local and global outliers.
Global outliers are the points that are very different from the data set, while local outliers may look not very different from the data set but inaccurate.
Although finding the outliers in each row is a difficult task, specifying their positions (columns) is even harder.
Thus we focus on detecting outliers that we can localize, and we give a method for finding outliers resulting from an unlogical combination of data within many columns and hard to localize.
% subsectionAccuracy metric (end)
\subsubsection{For nominal categorical columns} %
\label{par:frequncy_test}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{picture/logic_err.png}
    \caption{Two correlated columns from the dataset provided by Foyer.}
    \label{fig:logic_err}
\end{figure}
We use a frequency test to identify inaccurate data within the nominal categorical columns of the dataset.
The strategy of this test can be understood by considering an example from our dataset.
Figure \ref{fig:logic_err} represents sequences of couples from columns \texttt{ProductGroupDesc} and \texttt{Drive\_system} with the corresponding occurrence frequencies.
The basic idea is that outliers occur less frequently than inliers.
By examining the three \texttt{Drive\_system} associated with \texttt{Backhoe Loeders}, we can deduce that \texttt{All Wheel Drive} occurs remarkably less than the others so it may be an accuracy error.
Moreover, the high frequencies 11132 and 15639 imply that all couples occurring once are outliers.
They are spelling errors in column \texttt{ProductGroupDesc} present as variants of \texttt{Backoe Loeders}.

The test is a pattern-based method that searches for outlier/inlier patterns and employs the occurrence frequency as the outliers measure.
It applies to sufficiently correlated nominal clustered columns with a non-unique cluster.
The first step is to compute the matrix of correlations between applicable columns.
Then, for sufficiently two correlated columns \texttt{column\_a} and \texttt{column\_b} to each element of \texttt{column\_a}, we compute the occurrence frequencies of the associated elements in \texttt{column\_b}.
The test considers that low frequencies correspond to outliers.
It is an accuracy test able to specify the coordinates (row, column) of outliers.
Supplementary steps can be added to propose corrections for the spelling errors.
The test requires users’ specification of the outlier frequency threshold.
The pseudo-code defers to Appendix \ref{code:nominal_accuracy_error_by_frequencies}.
% subsection Frequency for logical errors (end)

\subsubsection{For numerical columns} % (fold)
\label{ssub:For numerical columns}
We used two methods to detect outliers in numerical columns.
The first search for order relation between columns and consider outliers as data not respecting this relation.
The latter is an unsupervised machine learning algorithm applied to all numerical columns at once to search for rows with possible outliers without specifying the locations (columns).

\paragraph{Order relation between columns.} % (fold)
\label{sub:Logical order relation}
Suppose that the data contains columns \texttt{BIRTH\_YEAR} and \texttt{DEATH\_YEAR}.
EAch value in \texttt{BIRTH\_YEAR} should be lower than the corresponding value in \texttt{DEATH\_YEAR}.
So \texttt{BIRTH\_YEAR} is related to \texttt{DEATH\_YEAR} by the order relation $(<)$, and the data not following this relation are corrupted.
Following this idea, we design an algorithm to search for the order relation $(<)$ between pair of numerical columns and indicate elements not following it.
We denote this test by the \textit{tendency test}.
First, we drop rows with missing values within columns \texttt{column\_a} and \texttt{column\_b}.
Then, we compute the ratio of the number of related elements (by the order relation $(<)$) to the column's length.
A "high" ratio implies that the order relation $(<)$ holds between \texttt{column\_a} and \texttt{column\_b}, and the elements not related by $(<)$ might be outliers.
A downside may be that the user needs to specify how "high" the ratio should be to confirm the relation between the columns.
It depends on the risk level we are ready to accept.
Prior knowledge of the data can help to fix this threshold.
The pseudo-code defer to Appendix \ref{code:order_relation_tendency_test}.
% subsection Logical order relation (end)

\paragraph{Outlier detection with unsupervised machine learning.} % (fold)
\label{sub:Outlier detection with machine learning}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.6\linewidth]{picture/lof_algo.png}
%     \caption{Local Outlier Factor.}
%     \label{fig:lof}
% \end{figure}
In this section, we try to find outliers rows using the numerical columns.
Specifically, we are looking for errors resulting from unlogical combinations between columns.
We used the local outlier factor (LOF) algorithm to identify the outlier rows.
LOF is an unsupervised machine learning outlier detection method that attempts to harness the idea of nearest neighbors for outlier detection, i.e., identify local outliers.
It is based on a concept of a local density, where locality is given by the $k$-nearest neighbors used to estimate the density.
Points having a lower density than their neighbors are considered to be outliers.
The local density is estimated by the typical distance at which a point can be ``reached" from its neighbors.
Reachability is a measure used to produce more stable results within clusters.
Due to the local approach, LOF can identify outliers in a data set that would not be outliers in another area of the data set.
For example, a point at a ``small" distance to a very dense cluster is an outlier, while a point within a sparse cluster might exhibit similar distances to its neighbors.

LOF doesn't apply to data containing missing values.
As most of the numerical columns of our data contain missing data, the first step is to deal with missing data by applying an imputation method.
Different approaches exist for imputing missing values \cite{corr_lede}.
For example,
\begin{enumerate}
    \item Deletion: removes all instances with missing values.
    \item Hot deck: missing data are filled with values from the same dataset.
    \item Imputation based on missing attributes: computes a new value from measures of central tendency as median, mode, mean...
    The computed value is used for filling the missing data.
    \item Imputation based on non-missing attributes: a classification or regression model is built from available data to fill the missing values.
\end{enumerate}
We chose the imputation method based on non-missing attributes using the $k$-nearest neighbors algorithm (KNN) provided by \href{https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html}{sklearn} \cite{Olga+al:2001}.
Missing values are imputed using the mean value from the $k$-nearest neighbors found in the data where two elements are neighbors if non-missing features are close.

To summarize, for numerical columns, this test uses the KNN algorithm first to replace missing values then, the obtained rows can be seen as high dimensional points to which we apply the LOF algorithm.
The downside of this test is that it indicates the rows that may contain outliers and not the position (i.e., column) of the outlier in the rows.
The pseudo-code defers to Appendix \ref{code:LOF_KNN_algo}.
% subsection  Outlier detection with machine learning (end)
\subsubsection{Outliers detection for numerical and nominal columns} % (fold)
\label{sub:Intra nominal extreme values}
For a nominal categorical column \texttt{column\_a} (with a high number of categories) and a numerical column \texttt{column\_b}, this test targets numerical outliers of {column\_a} within each category of {column\_b}.
For example, extreme values of the distribution of the prices of a specific car brand.
We proceed as follows.
Consider applicable nominal column{column\_a} and  numerical {column\_b}.
For each category of {column\_a} the goal is to identify outliers within the associated values in {column\_b}.
If {column\_b} is continuous, the extreme values test \ref{ssub:extreme_value_test} is applied while if it is discrete, the frequency test \ref{par:frequncy_test} is applied.
The pseudo-code defers to Appendix \ref{code:Mixed number text nominal logic algorithm}.

\section{Algorithmic scheme} % (fold)
\label{sec:Algorithmic scheme}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.17]{picture/algo_scheme.png}
    \label{fig:algo_scheme}
    \caption{Algorithmic scheme}
\end{figure}
Different error types and data types force us to use different methods and algorithms to detect bad data.
In light of this, we decided to split our algorithm into several standalone modules capable of being called independently of one another.
If the user wishes to use all the algorithms with standard parameters, we provided a Python Class called \texttt{data} in which the user can load the data and pre-process it if needed.
The algorithmic strategy is shown in Figure \ref{fig:algo_scheme} and the code is available upon request.
\texttt{data} accept an input of type .csv, .json, .sql, or .xlsx.
First, the input dataset is transformed into a DataFrame object of the Python toolbox \href{https://pandas.pydata.org/}{\texttt{pandas}}.
Then the columns are classified between date, numerical, and nominal.
The date columns are transformed into a uniform type.
The Class is designed of two passes as shown in Figure \ref{fig:algo_scheme}.
The first pass is used to get rid of duplicated rows and identify errors such as typographical errors, extreme values, and incomplete rows before (if wished) passing its results to the second method which will be a more in-depth check.
In the second pass, outlier detection by occurrence frequencies, and categorical extreme values are applied only to the remaining good data from the first pass.
Finally, the LOF algorithm and the tendency test are used.

The first method is also fully identifiable as it is a major consideration for the company to be able to check afterward if the error is true and where it is.
We wanted the code to be able to be used by users with basic knowledge and have room, if needed, to fine-tune each parameter independently if they have a deep knowledge of the subject.
Each algorithm used is fully described in Section \ref{sec:Starategy used to detect bad data}, and the pseudo-codes defer to the Appendix \ref{sec:Pseudo-code}.
% sectionStrategy used (end)

\section{Experiments} % (fold)
\label{sec:Experiments}
At the start of the challenge our correspondant at Foyer shared with us a dataset comprised of 100000 columns and 54 columns.
This dataset is a real life example of sales of agricultural machinery.
The types of column that we had were composed of numerical quantitative columns e.g. price, numerical category  e.g. Model Id but also verbal columns such as the state in which the sale took place.
Not all the columns were in a type directly usable such as \texttt{saledate} which were in a non classical format.

In this section we will discuss the type of errors caught by our algorithms, their strengths and shortcomings.

\subsection{First method}
In the first method we start by the redundency test described in Section \ref{sub:Redundancy} to find duplicated rows.
This method returns the index of the duplicated rows as we can see in Figure \ref{fig:exp_duplicate}.
For example, the first row of Figure \ref{fig:exp_duplicate}
shows that row 307 is a duplication of row 306 in the dataset.
For the computational time, the method took less than one second to search within 5 millions rows.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{picture/exp_duplicated.png}
    \label{fig:exp_duplicate}
    \caption{Sample from the detected errors featuring duplication}
\end{figure}

Second, the algorithm used for finding typographical errors \ref{typo} can be illustrated using the same method.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{picture/exp_duplicated_db.png}
    \label{fig:exp_duplicate_db}
    \caption{Sample from the original dataset.  A few columns were taken for tidiness's sake}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{picture/exp_typo.png}
    \label{fig:exp_typo}
    \caption{Sample from the detected errors featuring typographical errors}
\end{figure}
The figure \ref{fig:exp_typo} shows that in the column 'UsageBand' multiple instances of the values 'HIGH' and 'high' were detected as errors. Indeed after confirmation with our correspondant it is an induced error. The correct spelling is 'High', both with the affinity propagation and markov clustering we get this types of results. However, when the database gets bigger and more words starts to appear in a cluster, the base affinity propagation clustering fails to scale up both in terms of speed and performance because of the weak pairwise metric associated to the affinity propagation algorithm. The markov chain clustering methods on the other hand can be used even in high columns or row counts while still being tractable but give less desirable results in lower column counts. \\

For correcting numerical values while still being able to clearly tell in which column the error is we used the \ref{extreme} algorithm.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{picture/exp_extreme.png}
    \label{fig:exp_extreme}
    \caption{Sample from the detected errors featuring extreme values}
\end{figure}

On numerical column it flags as outliers extreme values using a z\_score method. On columns such as \texttt{YearMade} (of the machinery) it correctly flags absurd years such as negative years and far into the future years. Being a methods which flags when the value is well outside the mean it sometimes fails when considering columns which are made up of a continuous variable such as \texttt{SalePrice} but who has a latent variable such as \texttt{ModelId}. \\ Another type of column which might be a problem with \ref{extreme} is when the data is numerical but purely qualitative such as \texttt{datasource}. It is comprised of numbers but only two of them representing which office aggregated the data. For this type of error we implemented a check inside \ref{extreme} to abort if the ratio of uniqueness of the column is too low.\\
As for the speed and scalability of the method, on nearly 900000 cells it was in the order of a few seconds. On a larger dataset tested in collaboration of foyer it was similarly fast.

For the algorithm \ref{completeness} we can check \ref{fig:exp_completeness}, that it shows multiple exact same track type tractor-dozer down to the model id having on this subset of column less information.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{picture/exp_completeness_db.png}
    \label{fig:exp_completeness}
    \caption{Sample from the detected errors featuring completeness}
\end{figure}
On the other column there is also less information. Is is not stricto sensu an error but in our view it doesn't bring enough information to be useful.

\subsection{Second method}
A general trend for the second method algorithm is that they give harder to interpret results. Not all because of inability to target individual error cells but because trey rely on covariance and tendency between columns and as such need a deeper knowledge of the subject at hand to check if the reported error is indeed a true one.

The algorithm \ref{tendency} only operates on columns which are numerical in nature. A working example might be as shown in \ref{fig:exp_tendency} where we check \texttt{YearMade} against \texttt{saledate}. Up to at least 99,9\% in this case the order is preserved and is recognized by our algorithm. It sometimes fails in assessing the order due to the presence of lot's of outliers.
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{picture/exp_tendency.png}
    \label{fig:exp_tendency}
    \caption{Sample from the detected errors featuring tendency errors}
\end{figure}

In the algorithm \ref{outlier} we get to use the collective result of our previous methods to filter the flagged index. We leverage this to create a temporary higher data quality dataset which allows us to impute which otherwise would results in a low count of possible neighbors. In our case it appears to be (at least on the training set) an low risk low reward method, only flagging 10's indices in place of potentially hundreds for other methods. It however has a severe drawback that the imputation doesn't scale up with the number of rows very well. This methods can take up to a few minutes for a 900 000 data points.\\


One of the most interesting algorithm for us was the str and mixed type logic detector. To validate this kind of error we had to check the frequency and the validity of certain association between categories.
Typically if a certain type of agricultural machinery registered as a wheel loader but doesn't have the proper drive system it is usually a low frequency type of event and thus is detected by \ref{str_logic}.
Low frequency event, or for numerical table aberrant values can be detected using the mixed type logic algorithm \ref{mixed_logic}. For the verbal algorithm since it precomputes the correlation matrix it can be quite fast at low cells counts but costly if working on a billion data point. Even though the mixed type doesn't use this correlation matrix it can still be slow at similar numbers of inputs.
As we will discuss in the next section optimisation and adaptations can be improved upon.
% sectionExperiments (end)

\section{Discussions} % (fold)
\label{sec:Discussions}
We tested the algorithm on a data set provided by Foyer, after comparing the flagged errors with the known errors injected by the company, it showed a success rate of 36\%, we made the choice of using very strict thresholds to avoid false positives to not flood the user with information, the major positive component is the fact that the algorithm runs relatively fast. We also proceeded to a quick test on one of the data sets of the company and the test were very encouraging as well. \
The next step would be to control the number of false positives that we get, one way to proceed would be to improve the choice of the thresholds, also for the \texttt{check\_mixt\_logic()} to run a correlation test between columns in order to only consider the columns that are sufficiently correlated.
in addition regarding the typo algorithm, we observed that if we run the Affinity Propagation algorithm twice, the first time on the whole data set of words and the second time on the proposed clusters, it highly improves the results and we end up getting a better clustering of the words, which makes it easier to detect spelling errors and suggest a correction to the user. \
Another axis of research, that we did not have time to explore is about the classification of the missing values (nan's) in the data set, we wanted to implement a solution that would help sort between the null value  missing completely at random, missing at random and missing not at random, which is an outlier types of missing values. Detecting which kind of missing value is important in the imputation phase of pre-processing the data.
% section Discussions (end)

\section*{Acknowledgments} % (fold)
\label{sec:Acknowledgments}
We would like to thank AMIES for this beautiful opportunity and Foyer for providing us with the technical support and supervising (special thanks to Alexandre Hotton).


% sectionAcknowledgments (end)

\begin{thebibliography}{999}

    \bibitem{pan_cos_chen}
    Guansong Pang, Longbing Cao and Ling Chen:
    \emph{Outlier Detection in Complex nominal Data
        by Modelling the Feature Value Couplings.}

    \bibitem{amazon}
    Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Celikel, Felix Biessmann
    \emph{Automating Large-Scale Data Quality Verification}

    \bibitem{dai_yosh_pars}
    Wei Dai, Kenji Yoshigoe and William Parsley
    \emph{Improving Data Quality Through Deep Learning and Statistical Models}

    \bibitem{corr_lede}
    David Camilo Corrales, Agapito Ledezma and Juan Carlos Corrales
    \emph{From Theory to Practice: A Data Quality Framework
        for Classification Tasks}

    \bibitem{Hawking}
    Simon Hawkins, Hongxing He, Graham Williams, Rohan Baxte:
    \emph{Outlier Detection Using Replicator Neural Networks}
    \bibitem{Olga+al:2001}
    Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie, Robert Tibshirani, David Botstein, Russ B. Altman:
    \emph{Missing value estimation methods for DNA microarrays, Bioinformatics, Volume 17, Issue 6, June 2001}

\end{thebibliography}
\appendix
\section{Pseudo-code} % (fold)
\label{sec:Pseudo-code}
\subsection{Typographical error algorithm}
\label{sub:Typographical error algorithm}
\begin{algorithm}[H]
    \SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
    \SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{A column of a DataFrame, a minimum frequency threshold, A specified method to deal with clustering}
    \Output{Index of the DataFrame containing typographical errors}
    \BlankLine

    list\_incorrect$\leftarrow$ [~]\;
    words $\leftarrow$ get unique element from input column\;
    \If{method is affinity propagation}
    {
        lev\_similarity $\leftarrow$ \lForEach{element $w1$ in word, element $w2$ in word}{SequenceMatcher(w1, w2)}
        affprop $\leftarrow$ instance AffinityPropagation\;
        affprop\_label $\leftarrow$ affprop.fit(lev\_similarity).labels \;
        \eIf{affprop\_label is empty}
        {
            return list\_incorrect\;
        }
        {\For{cluster in affprop\_label}
            {list\_incorrect $\leftarrow$ list\_incorrect + incorrect\_grammar(column, cluster, threshold)\;}
        }
        return list\_incorrect
    }
    \Else{
        X $\leftarrow$ instance Vectorizer and transform words into it\;
        dict\_cluster $\leftarrow$ get clusters from MarkovClustering(X)\;
        \If{dict\_cluster is empty}
        {return list\_incorrect}
    }
    \caption{Typographical checking\label{typo}}
\end{algorithm}



\subsection{Duplicate errors algorithm}
\label{code:Duplicate errors algorithm}
\begin{algorithm}[H]
    \SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
    \SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{A DataFrame}
    \Output{Index of the DataFrame duplicated, index of first duplicate}
    \BlankLine
    boolean\_idx $\leftarrow$ duplicated(dataframe, omit\_first)\;
    boolean\_first $\leftarrow$ duplicated(dataframe, all)\;

    index\_true $\leftarrow$ boolean\_idx(boolean\_idx) and boolean\_first(boolean\_first)\;

    return index(boolea\_idx == True), index\_true
    \caption{Duplication checking\label{duplicate}}
\end{algorithm}



\subsection{Extreme value algorithm}
\label{code:extreme_values_numerical}
\begin{algorithm}[H]
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{A column of a DataFrame, a minimum frequency threshold, the uniqueness ratio of the column, a standard deviation multiplier, 2 uniqueness thresholds}
    \Output{Index of the DataFrame containing extreme values}
    \BlankLine

    \uIf{uniqueness $\geq$ threshold\_1 or uniqueness $\leq$ threshold\_2}{return [~] \;}
    \Else{
        mean $\leftarrow$ mean(column)\;
        std $\leftarrow$ std(column)\;

        upper\_bound $\leftarrow$ mean + threshold\_std * std\;
        lower\_bound $\leftarrow$ mean - threshold\_std * std\;

        idx $\leftarrow$ column where element is $\geq$ lower\_bound and $\leq$ upper\_bound\;
        return idx\;
    }
    \caption{Extreme value checking\label{extreme}}
\end{algorithm}



\subsection{Row completeness algorithm}
\label{code:completeness}
\begin{algorithm}[H]
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{A DataFrame, a minimum threshold of empty values for rows, a minimum threshold of empty values for rows when low information columns are removed, a minimum threshold of empty values for columns}
    \Output{Index of the DataFrame containing rows with low completeness}
    \BlankLine
    mean\_none\_row1 $\leftarrow$ mean(NA in dataframe.rows)\;
    index\_1 $\leftarrow$ index(rows $\geq$ thresh\_row\_1 NA)\;
    \BlankLine
    mean\_none\_col $\leftarrow$ mean(NA in dataframe.columns)\;
    index\_col $\leftarrow$ index(columns $\geq$ thresh\_col NA)\;
    \BlankLine
    \For{idx in index\_col}{drop column[idx] of dataframe}
    \BlankLine
    mean\_none\_row2 $\leftarrow$ mean(NA in dataframe.rows)\;
    index\_2 $\leftarrow$ index(rows $\geq$ thresh\_row\_2 NA)\;
    \BlankLine
    idx = index\_1 and index\_2\;
    return idx
    \caption{Row completeness checking\label{completeness}}
\end{algorithm}



\subsubsection{Tendency algorithm}
\label{code:order_relation_tendency_test}
\begin{algorithm}[H]
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{A DataFrame, a minimum threshold for an observed order between variable to become a rule to discriminate outliers upon.}
    \Output{Index and column of the DataFrame containing rows which breaks the tendency}
    \BlankLine

    dict\_tendency $\leftarrow$ \{\}\;

    \For{column1 in numerical\_columns(dataframe)}{
        \For{column2 in numerical\_columns(dataframe)}{
            dataframe $\leftarrow$ drop(dataframe(column1, column2), any rows with NA)\;
            tendency $\leftarrow$ $\frac{size(dataframe(column1) \leq dataframe(column2))}{size(dataframe)}$\;
            \BlankLine
            \If{tendency $\geq$ tendency\_threshold}{
                \If{size(dataframe(column1) $\geq$ dataframe(column2)) $\geq$ 1}{
                    dict\_tendency[(column1, column2)] $\leftarrow$ index(dataframe(column1) $\geq$ dataframe(column2))\;
                }
            }
        }
    }
    return dict\_tendency
    \caption{Tendency checking\label{tendency}}
\end{algorithm}
\subsubsection{Whole row outlier detection algorithm}
\label{code:LOF_KNN_algo}
\begin{algorithm}[H]
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{A DataFrame}
    \Output{Index of the DataFrame containing rows which are deemed to be outliers using the lof method}
    \BlankLine

    \If{dataframe has NA}{
        dataframe $\leftarrow$ KNNImputer(dataframe)\;
    }
    \BlankLine
    clf $\leftarrow$ instancize LocalOutlierFactor\;
    prediction $\leftarrow$ clf.fit\_predict(dataframe)\;
    idx $\leftarrow$ index(prediction == outlier)\;
    \BlankLine
    return idx
    \caption{Whole row outlier checking\label{outlier}}
\end{algorithm}

\subsubsection{Text nominal logic algorithm}
\label{code:nominal_accuracy_error_by_frequencies}
\begin{algorithm}[H]
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{A DataFrame, a threshold for of the uniqueness for columns, a threshold for the number of time a nominal error happens}
    \Output{Index of the DataFrame and two columns in which a nominal error is made}
    \BlankLine

    dataframe $\leftarrow$ dataframe(index not rejected by first method)\;

    col\_names $\leftarrow$ [~]\;
    idxes $\leftarrow$ [~]\;
    \For{column1 in verbal\_column(dataframe)}{
        \For{column2 in verbal\_correlated\_columns(dataframe, column1)}{
            \If{column1 not == column2 and uniqueness(column1) $\leq$ thres\_uniqueness and uniqueness(column2) $\leq$ thres\_uniqueness}{
                occurences $\leftarrow$ get\_contingency\_table(dataframe(column1), dataframe(column2))\;
                elements $\leftarrow$ dataframe((column1, column2) not NA)\;
                \For{elem in unique(elements(column1))}{
                    \For{(e, value) in (occurences(elem), index(occurences(elem))}{
                        \If{e $\leq$ thres\_error}{
                            idxes $\leftarrow$ idxes $+$ elements(column2 where index = elements(column2) == value and elements(column1) == elem)\;
                            col\_names $\leftarrow$ col\_names $+$ (column1, column2)
                        }
                    }
                }
            }
        }
    }

    return idxes, col\_names
    \caption{Text nominal logic error checking\label{str_logic}}
\end{algorithm}




\subsubsection{Mixed number text nominal logic algorithm}
\label{code:Mixed number text nominal logic algorithm}
\begin{algorithm}[H]
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{A DataFrame, a threshold for of the uniqueness for verbal columns, a threshold for numerical columns, a standard deviation multiplier}
    \Output{Index of the DataFrame and two columns in which a nominal error is made}
    \BlankLine

    dataframe $\leftarrow$ dataframe(index not rejected by first method)\;

    col\_names $\leftarrow$ [~]\;
    idxes $\leftarrow$ [~]\;
    list\_seen $\leftarrow$ [~]\;

    \For{column1 in verba\_columns(dataframe)}{
        \If{uniqueness(column1) $\leq$ threshold\_verbal}{
            \For{column2 in numerica\_columns(dataframe)}{
                \If{(column1, column2) not in list\_seen and uniqueness(column2) $\geq$ thresol\_numerical}{
                    elements $\leftarrow$ dataframe((column1, column2) not NA)\;
                    \For{elem in unique(elements(column1))}{
                        array\_values $\leftarrow$ elements(column2 where elements(column1) == elem)\;
                        \If{array\_values not empty}{
                            index\_outliers $\leftarrow$ Algorithm\_2(array\_values, 0.1, thresh\_std)\;
                            \If{index\_outliers not empty}{
                                value\_outliers $\leftarrow$ array\_values(index\_outliers)\;
                                idxes $\leftarrow$ idxes $+$ index(element(column2 where element(column2) == value\_outliers))\;
                                col\_names $\leftarrow$ col\_names $+$ (column1, column2)\;
                            }
                        }
                    }
                    list\_seen $\leftarrow$ list\_seen + (column1, column2)
                }
            }
        }
    }

    return idxes, col\_names

    \caption{Mixed number text nominal logic error checking\label{mixed_logic}}
\end{algorithm}


% sectionAlgorithm (end)

\end{document}
